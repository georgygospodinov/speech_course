{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is tested and supposted to be used in kaggle notebbok.\n",
    "\n",
    "https://www.kaggle.com/code/alexandrmaximenko/audiollm-hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "from transformers import AutoModel, AutoProcessor, WhisperFeatureExtractor, WhisperModel, AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.generation import GenerationConfig\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import IPython\n",
    "import gc\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import torch.nn as nn\n",
    "\n",
    "# There might me Errors like \n",
    "# \"AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\"\n",
    "# It's okay and won't effect notebook executio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# downloading helping visualizations\n",
    "! gdown https://drive.google.com/uc?id=1k8LeWGhsn1fTXbrn9JV08lKUTOJeoEwz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework: Audio Language Model Training and Evaluation (10 points)\n",
    "\n",
    "In this homework, you will implement AudioLLM training pipeline and check it correctness with overfitting experiment.\n",
    "\n",
    "Then, you'll take pretrained checkpoint and config for AudioLLM and write code for it's evaluation.\n",
    "\n",
    "\n",
    "**Good luck! üöÄ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation Pipeline (2 points)\n",
    "Implement a QA-data generation pipeline from ASR (Automatic Speech Recognition) dataset\n",
    "- Start with ASR dataset containing `(audio, transcript)` pairs\n",
    "- Use the transcript to prompt an instruction-tuned LLM to generate question based on the transcript\n",
    "- Use the transcript & question to prompt an instruction-tuned LLM to generate answer for this question based on transcript\n",
    "- Create `(audio, question, anwer)` pairs for training\n",
    "\n",
    "**Expected output**: A data generation script that transforms ASR data into QA-dataset + some generated samples\n",
    "\n",
    "Expected sample format:\n",
    "```\n",
    "{\n",
    "  \"audio_path\": \"some_audio_path.flac\",\n",
    "  \n",
    "  \"transcript\": \"So today we're going to talk about how to prepare for a marathon if you only have twelve weeks...\",\n",
    "  \n",
    "  \"question\": \"How long does the speaker say you have to prepare for the marathon?\",\n",
    "  \n",
    "  \"answer\": \"Twelve weeks.\"\n",
    "}\n",
    "```\n",
    "\n",
    "#### Grading Criteria:\n",
    "* **2 points:** Completed *<TODO>* code in the `QADataGeneratorHF` + meaningfull examples of questions/answers in the last cell, completed `check_response` if needed\n",
    "* **1 point:**: Completed *<TODO>* code in the `QADataGeneratorHF` + many bugs in examples like incorrect format / model refusals\n",
    "* **0 points:** lack of meaningful data examples / unexecutable code\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Download librispeech dev-clean data\n",
    "! wget https://openslr.elda.org/resources/12/dev-clean.tar.gz \\\n",
    "  && tar -xf dev-clean.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "DEV_CLEAN_ROOT = Path(\"/kaggle/working/LibriSpeech/dev-clean\")\n",
    "\n",
    "\n",
    "# Parse all transcript files into a dict: utt_id -> transcription\n",
    "def load_transcripts(root: Path) -> dict:\n",
    "    transcripts = {}\n",
    "    for trans_file in root.rglob(\"*.trans.txt\"):\n",
    "        with open(trans_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if not parts:\n",
    "                    continue\n",
    "                utt_id = parts[0]              # e.g. \"84-121123-0000\"\n",
    "                text = \" \".join(parts[1:])\n",
    "                transcripts[utt_id] = text\n",
    "    return transcripts\n",
    "\n",
    "transcripts = load_transcripts(DEV_CLEAN_ROOT)\n",
    "print(\"Loaded transcripts:\", len(transcripts))\n",
    "\n",
    "# Create JSONL with one entry per audio file\n",
    "output_json = Path(\"/kaggle/working/dev-clean.json\")\n",
    "librispeech_data = []\n",
    "for flac_path in sorted(DEV_CLEAN_ROOT.rglob(\"*.flac\")):\n",
    "    utt_id = flac_path.stem  # filename without .flac\n",
    "    text = transcripts.get(utt_id)\n",
    "    record = {\n",
    "        \"audio_path\": str(flac_path),    # full path inside Kaggle FS\n",
    "        \"transcription\": text,\n",
    "    }\n",
    "    librispeech_data.append(record)\n",
    "    \n",
    "with output_json.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(librispeech_data, f)\n",
    "        \n",
    "\n",
    "print(\"Wrote JSONL to:\", output_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Example of data sample\n",
    "index = torch.randint(len(librispeech_data), (1,))\n",
    "print('Data sample transcription:\\n', librispeech_data[index]['transcription'])\n",
    "IPython.display.Audio(librispeech_data[index]['audio_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BASE_SYSTEM_MESSAGE = (\n",
    "    \"You are a helpful assistant. \"\n",
    "    \"Keep your responses concise ‚Äî maximum 50 words or 2‚Äì3 sentences.\"\n",
    ")\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class QADataGeneratorHF:\n",
    "    def __init__(self, model_path: str = \"Qwen/Qwen2.5-1.5B-Instruct\"):\n",
    "        print(f\"Loading tokenizer: {model_path}\")\n",
    "        self.tokenizer = None #<TODO>\n",
    "\n",
    "        print(f\"Loading transformers model: {model_path}\")\n",
    "        self.device = DEVICE\n",
    "        self.model = None #<TODO>\n",
    "        self.model.eval()\n",
    "\n",
    "        self.generation_kwargs = dict(\n",
    "            temperature=0.8,\n",
    "            top_p=0.9,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "        )\n",
    "\n",
    "        self.system_message = BASE_SYSTEM_MESSAGE\n",
    "\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def generate_single(self, transcription: str) -> str:\n",
    "        pass #<TODO>\n",
    "        return text\n",
    "\n",
    "    def generate_batch(self, transcriptions: list[str]) -> list[str]:\n",
    "        \"\"\"Generate responses for a list of transcriptions with progress bar.\"\"\"\n",
    "        responses = []\n",
    "        for transcription in tqdm(transcriptions, desc=\"Generating responses\"):\n",
    "            response = self.generate_single(transcription)\n",
    "            responses.append(response)\n",
    "        \n",
    "        return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "generator = QADataGeneratorHF(\"Qwen/Qwen2.5-1.5B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 100\n",
    "librispeech_data_sample = librispeech_data[:NUM_SAMPLES]\n",
    "transcriptions = [sample['transcription'] for sample in librispeech_data_sample]\n",
    "\n",
    "generated_responses = generator.generate_batch(transcriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Check your instruct data samples\n",
    "index = torch.randint(NUM_SAMPLES, (1,))\n",
    "print('Transcription example: ', librispeech_data_sample[index]['transcription'])\n",
    "print('='*100)\n",
    "print('Question example: ', generated_responses[index]['question'])\n",
    "print('='*100)\n",
    "print('Answer example: ', generated_responses[index]['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## Add simple filtering\n",
    "\n",
    "def check_response(response: str) -> bool:\n",
    "    return True #<TODO>\n",
    "\n",
    "generated_samples = []\n",
    "rejected_samples = []\n",
    "\n",
    "for data_sample, response in zip(librispeech_data_sample[:NUM_SAMPLES], generated_responses):\n",
    "    if check_response(response):\n",
    "        data_sample.update(response)\n",
    "        generated_samples.append(data_sample)\n",
    "    else:\n",
    "        data_sample.update(response)\n",
    "        rejected_samples.append(data_sample)\n",
    "\n",
    "print('Number of samples before filtering: ', NUM_SAMPLES)\n",
    "print('Number of samples after filtering: ', len(generated_samples))\n",
    "with open('instruct_data.json', 'w') as file:\n",
    "    json.dump(generated_samples, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Check rejected samples\n",
    "index = torch.randint(len(rejected_samples), (1,))\n",
    "print('Transcription example: ', rejected_samples[index]['transcription'])\n",
    "print('Response example: ', rejected_samples[index]['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Check good samples\n",
    "index = torch.randint(len(generated_samples), (1,))\n",
    "print('Transcription example: ', generated_samples[index]['transcription'])\n",
    "print('='*100)\n",
    "print('Question example: ', generated_samples[index]['question'])\n",
    "print('='*100)\n",
    "print('Answer example: ', generated_samples[index]['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Grading Criteria:\n",
    "* **2 points:** Completed *<TODO>* code in the `QADataGeneratorHF` + meaningfull examples of questions/answers in the last cell, completed `check_response` if needed\n",
    "* **1 point:**: Completed *<TODO>* code in the `QADataGeneratorHF` + many bugs in examples like incorrect format / model refusals\n",
    "* **0 points:** lack of meaningful data examples / unexecutable code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2. AudioLLM Model & Dataset & Overfitting (4 points)\n",
    "\n",
    "In this section you'll need to: \n",
    "* Implement AudioLLM model with architecture, similar to UltraVox.\n",
    "\n",
    "* Implement dataset class for this model based on QA-samples you've generated earlier\n",
    "\n",
    "* Run overfitting experiment with your model and your dataset\n",
    "\n",
    "\n",
    "#### Grading Criteria:\n",
    "* **+2 points:** Completed *<TODO>* code in the `AudioAdapter` and in the `AudioLLM`, `AudioEmbeddingInsertionTests` passed\n",
    "* **+1 point:** Completed *<TODO>* code in the `AudioInstructDataset` + demonstration of dataset samples\n",
    "* **+1 point:** Reaching `loss < 0.2` on your overfit experiment\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let start with model.\n",
    "\n",
    "**Model components**:\n",
    "  - Audio encoder - Whisper\n",
    "  - Audio Adapter - Linear with subsampling\n",
    "  - Language model\n",
    "\n",
    "Your first task - to fill **`#<TODO>`** parts in AudioAdapter and AudioLLM classes.\n",
    "\n",
    "##### Grading Criteria:\n",
    "* **2 points:** Completed *<TODO>* code in the `AudioAdapter` and in the `AudioLLM`, `AudioEmbeddingInsertionTests` passed\n",
    "* **1 point:** Completed *<TODO>* code in the `AudioAdapter`, `AudioEmbeddingInsertionTests` failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for the Audio LLM model.\"\"\"\n",
    "    \n",
    "    whisper_model: str = \"openai/whisper-small\"\n",
    "    llm_model: str = \"Qwen/Qwen3-0.6B\"\n",
    "    \n",
    "    # Audio adapter\n",
    "    adapter_hidden_dim: int = 1024 # corresponding to llm input dimension\n",
    "    adapter_num_layers: int = 2\n",
    "    adapter_dropout: float = 0.1\n",
    "    subsample_factor: int = 4\n",
    "\n",
    "    # Training strategy\n",
    "    freeze_whisper: bool = True  # Freeze Whisper encoder\n",
    "    freeze_llm: bool = True  # Set to True for adapter-only training (no LoRA needed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AudioAdapter(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        whisper_dim: int,\n",
    "        llm_dim: int,\n",
    "        hidden_dim: int = 2048,\n",
    "        num_layers: int = 2,\n",
    "        dropout: float = 0.1,\n",
    "        subsample_factor: int = 1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Simple MLP Adapter with configurable subsample_factor\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.subsample_factor = subsample_factor\n",
    "\n",
    "        # Set input_dim according to subsample_factor and whisper_dim\n",
    "        input_dim = None #<TODO>\n",
    "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.LayerNorm(hidden_dim),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "            for _ in range(num_layers - 1)\n",
    "        ])\n",
    "\n",
    "        self.output_proj = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Linear(hidden_dim, llm_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, whisper_features: torch.Tensor) -> torch.Tensor:\n",
    "        if self.subsample_factor > 1:\n",
    "            batch_size, seq_len, dim = whisper_features.shape\n",
    "            remainder = seq_len % self.subsample_factor\n",
    "            if remainder != 0:\n",
    "                whisper_features = whisper_features[:, :-remainder, :]\n",
    "                seq_len = whisper_features.shape[1]\n",
    "            new_seq_len = seq_len // self.subsample_factor\n",
    "            x = whisper_features.reshape(batch_size, new_seq_len, dim * self.subsample_factor)\n",
    "        else:\n",
    "            x = whisper_features\n",
    "\n",
    "        # Applying input projection\n",
    "        x = None #<TODO>\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = x + layer(x)\n",
    "\n",
    "        # Applying output projection\n",
    "        x = None #<TODO>\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "AUDIO_TOKEN = \"<|audio_token|>\"\n",
    "\n",
    "class AudioLLM(nn.Module):\n",
    "    \"\"\"\n",
    "    Audio Language Model combining Whisper encoder with a text LLM.\n",
    "    \n",
    "    Architecture:\n",
    "    1. Whisper encoder processes audio\n",
    "    2. Audio adapter projects features to LLM space\n",
    "    3. LLM generates text conditioned on audio features\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.model_config = config\n",
    "        \n",
    "        # Load Whisper encoder\n",
    "        print(f\"Loading Whisper model: {config.whisper_model}\")\n",
    "        whisper = WhisperModel.from_pretrained(config.whisper_model)\n",
    "        self.whisper_encoder = whisper.encoder\n",
    "        whisper_dim = self.whisper_encoder.config.d_model\n",
    "        \n",
    "        if config.freeze_whisper:\n",
    "            for param in self.whisper_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(\"Whisper encoder frozen\")\n",
    "        \n",
    "        # Load LLM\n",
    "        print(f\"Loading LLM: {config.llm_model}\")\n",
    "        self.llm = AutoModelForCausalLM.from_pretrained(\n",
    "            config.llm_model,\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "\n",
    "        # Adding special audio token\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.llm_model)\n",
    "        self.tokenizer.add_special_tokens({\"additional_special_tokens\": [AUDIO_TOKEN]})\n",
    "        self.audio_token_id = self.tokenizer(AUDIO_TOKEN, add_special_tokens=False).input_ids[0]\n",
    "        \n",
    "        # Ensure tokenizer has pad token\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "        llm_dim = self.llm.config.hidden_size\n",
    "        \n",
    "        # Audio adapter (initialize before freezing/LoRA)\n",
    "        print(\"Initializing audio adapter\")\n",
    "        self.audio_adapter = AudioAdapter(\n",
    "            whisper_dim=whisper_dim,\n",
    "            llm_dim=llm_dim,\n",
    "            hidden_dim=config.adapter_hidden_dim,\n",
    "            num_layers=config.adapter_num_layers,\n",
    "            dropout=config.adapter_dropout,\n",
    "            subsample_factor=config.subsample_factor\n",
    "        )\n",
    "        \n",
    "        # Freeze LLM if specified\n",
    "        if config.freeze_llm:\n",
    "            for param in self.llm.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(\"‚úì LLM fully frozen (only adapter will be trainable)\")\n",
    "        \n",
    "        # Set config attribute to LLM's config (required by Trainer)\n",
    "        self.config = self.llm.config\n",
    "        \n",
    "    def get_device(self):\n",
    "        return next(self.llm.parameters()).device\n",
    "\n",
    "    def get_dtype(self):\n",
    "        return next(self.llm.parameters()).dtype\n",
    "    \n",
    "    def encode_audio(self, audio_values: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode audio through Whisper and adapter.\n",
    "        For simplification, we are using fixed 30s-padding \n",
    "        as in original whisper and won't use audios longer \n",
    "        than 30 seconds in our training.\n",
    "        \n",
    "        Args:\n",
    "            audio_values: [batch_size, audio_length]\n",
    "        Returns:\n",
    "            audio_embeds: [batch_size, num_audio_tokens, llm_dim]\n",
    "        \"\"\"\n",
    "        # Ensure audio is on correct device and dtype\n",
    "        whisper_dtype = self.get_dtype()\n",
    "        whisper_device = self.get_device()\n",
    "        audio_values = audio_values.to(device=whisper_device, dtype=whisper_dtype)\n",
    "        \n",
    "        # Whisper encoding\n",
    "        with torch.no_grad() if self.model_config.freeze_whisper else torch.enable_grad():\n",
    "            whisper_outputs = self.whisper_encoder(audio_values)\n",
    "            whisper_features = whisper_outputs.last_hidden_state\n",
    "        \n",
    "        # Project to LLM space\n",
    "        audio_embeds = self.audio_adapter(whisper_features)\n",
    "        return audio_embeds\n",
    "    \n",
    "    @staticmethod\n",
    "    def insert_audio_embeds(\n",
    "        audio_embeds: torch.Tensor, \n",
    "        text_embeds: torch.Tensor, \n",
    "        input_ids: torch.Tensor,\n",
    "        labels: torch.Tensor | None,\n",
    "        audio_token_id: int\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor | None]:\n",
    "        \"\"\"\n",
    "        Insert audio embeddings into text embeddings at positions marked by audio_token_id.\n",
    "        \n",
    "        Args:\n",
    "            audio_embeds: [batch_size, num_audio_tokens, llm_dim]\n",
    "            text_embeds: [batch_size, text_seq_len, llm_dim]\n",
    "            input_ids: [batch_size, text_seq_len] - contains audio_token_id at positions to replace\n",
    "            labels: [batch_size, text_seq_len] - optional labels to adjust\n",
    "            audio_token_id: int - ID of the audio token placeholder\n",
    "        \n",
    "        Returns:\n",
    "            combined_embeds: [batch_size, new_seq_len, llm_dim] where audio_token_id positions are replaced\n",
    "            combined_labels: [batch_size, new_seq_len] with -100 at audio positions (if labels provided)\n",
    "        \"\"\"\n",
    "        pass #<TODO>\n",
    "        return combined_embeds, combined_labels\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        audio_values: torch.Tensor,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor | None = None,\n",
    "        labels: torch.Tensor | None = None\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Forward pass for training.\n",
    "        \n",
    "        Args:\n",
    "            audio_values: [batch_size, audio_length]\n",
    "            input_ids: [batch_size, text_seq_len] - text tokens\n",
    "            attention_mask: [batch_size, text_seq_len]\n",
    "            labels: [batch_size, text_seq_len] - for computing loss\n",
    "        \"\"\"\n",
    "        batch_size = audio_values.shape[0]\n",
    "        \n",
    "        # Encode audio\n",
    "        audio_embeds = self.encode_audio(audio_values)  # [B, audio_tokens, dim]\n",
    "        num_audio_tokens = audio_embeds.shape[1]\n",
    "        \n",
    "        # Get text embeddings\n",
    "        text_embeds = self.llm.get_input_embeddings()(input_ids)  # [B, text_tokens, dim]\n",
    "        \n",
    "        # Insert audio embeddings and adjust labels\n",
    "        combined_embeds, combined_labels = AudioLLM.insert_audio_embeds(\n",
    "            audio_embeds, text_embeds, input_ids, labels, self.audio_token_id\n",
    "        )\n",
    "        \n",
    "        # Create combined attention mask\n",
    "        if attention_mask is not None:\n",
    "            # Prepend ones for audio embeddings to existing attention mask\n",
    "            audio_attention = torch.ones(\n",
    "                batch_size, num_audio_tokens,\n",
    "                device=audio_embeds.device,\n",
    "                dtype=attention_mask.dtype\n",
    "            )\n",
    "            combined_attention = torch.cat([audio_attention, attention_mask], dim=1)\n",
    "        else:\n",
    "            combined_attention = None\n",
    "        \n",
    "        # Forward through LLM\n",
    "        outputs = self.llm(\n",
    "            inputs_embeds=combined_embeds,\n",
    "            attention_mask=combined_attention,\n",
    "            labels=combined_labels,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio Embedding Insertion Method\n",
    "\n",
    "In the whole AudioLLM class you'll need to implement only this method, all other methods are implemented already.\n",
    "\n",
    "This method is marked as static to make automatic testing easier, don't unmark it.\n",
    "\n",
    "##### Overview\n",
    "\n",
    "This method is the core of multimodal fusion in AudioLLM. It takes:\n",
    "- **Text embeddings** with special audio placeholder tokens\n",
    "- **Audio embeddings** from the audio encoder\n",
    "- **Input IDs** showing where placeholders are located\n",
    "\n",
    "And produces a **combined embedding sequence** where audio placeholders are replaced by actual audio embeddings.\n",
    "\n",
    "##### Algorithm Steps\n",
    "\n",
    "1. **Locate placeholders**: Find positions in `input_ids` that contain `audio_token_id`\n",
    "2. **Replace with audio**: At each placeholder position, insert ALL audio embeddings\n",
    "3. **Keep text embeddings**: All other positions keep their original text embeddings\n",
    "4. **Mask labels**: Set labels to `-100` at audio positions (ignored in loss calculation)\n",
    "5. **Pad sequences**: Ensure all batch items have the same length\n",
    "\n",
    "##### Visual Example\n",
    "\n",
    "See the diagram below for how the insertion works:\n",
    "![](embeddings_visualization.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AudioEmbeddingInsertionTests:\n",
    "    \"\"\"Test suite for insert_audio_embeds method.\"\"\"\n",
    "    \n",
    "    def __init__(self, insert_audio_embeds_func):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            insert_audio_embeds_func: The static method to test\n",
    "        \"\"\"\n",
    "        self.insert_func = insert_audio_embeds_func\n",
    "        self.audio_token_id = 99999\n",
    "        self.llm_dim = 128\n",
    "        \n",
    "    def test_single_audio_token_per_sample(self):\n",
    "        \"\"\"Test with exactly one audio placeholder per sample.\"\"\"\n",
    "        print(\"\\n‚úì Test 1: Single audio placeholder per sample\")\n",
    "        \n",
    "        batch_size = 2\n",
    "        text_seq_len = 5\n",
    "        num_audio_tokens = 15  # Typical for Ultravox\n",
    "        \n",
    "        audio_embeds = torch.randn(batch_size, num_audio_tokens, self.llm_dim)\n",
    "        text_embeds = torch.randn(batch_size, text_seq_len, self.llm_dim)\n",
    "        \n",
    "        # Each sample has exactly 1 audio placeholder at different positions\n",
    "        input_ids = torch.tensor([\n",
    "            [1, 2, self.audio_token_id, 3, 4],      # Audio at position 2\n",
    "            [5, 6, 7, self.audio_token_id, 8]       # Audio at position 3\n",
    "        ])\n",
    "        \n",
    "        labels = torch.tensor([\n",
    "            [10, 20, 30, 40, 50],\n",
    "            [60, 70, 80, 90, 100]\n",
    "        ])\n",
    "        \n",
    "        combined_embeds, combined_labels = self.insert_func(\n",
    "            audio_embeds, text_embeds, input_ids, labels, self.audio_token_id\n",
    "        )\n",
    "        \n",
    "        # Expected: 5 - 1 + 15 = 19 tokens per sequence\n",
    "        expected_len = text_seq_len - 1 + num_audio_tokens\n",
    "        assert combined_embeds.shape == (batch_size, expected_len, self.llm_dim), \\\n",
    "            f\"Expected shape {(batch_size, expected_len, self.llm_dim)}, got {combined_embeds.shape}\"\n",
    "        assert combined_labels.shape == (batch_size, expected_len), \\\n",
    "            f\"Expected labels shape {(batch_size, expected_len)}, got {combined_labels.shape}\"\n",
    "        \n",
    "        # Check audio positions have -100 labels\n",
    "        # Batch 0: audio at positions 2-16 (15 tokens)\n",
    "        for i in range(2, 2 + num_audio_tokens):\n",
    "            assert combined_labels[0, i] == -100, \\\n",
    "                f\"Audio position {i} should have -100 label, got {combined_labels[0, i]}\"\n",
    "        \n",
    "        # Batch 1: audio at positions 3-17 (15 tokens)\n",
    "        for i in range(3, 3 + num_audio_tokens):\n",
    "            assert combined_labels[1, i] == -100, \\\n",
    "                f\"Audio position {i} should have -100 label, got {combined_labels[1, i]}\"\n",
    "        \n",
    "        # Check non-audio positions keep original labels\n",
    "        assert combined_labels[0, 0] == 10, \"First position should keep original label\"\n",
    "        assert combined_labels[0, 1] == 20, \"Second position should keep original label\"\n",
    "        assert combined_labels[0, 17] == 40, \"Position after audio should keep original label\"\n",
    "        \n",
    "        print(f\"  ‚úì Output shape: {combined_embeds.shape}\")\n",
    "        print(f\"  ‚úì Labels shape: {combined_labels.shape}\")\n",
    "        print(f\"  ‚úì Audio positions (15 tokens) masked with -100\")\n",
    "        print(f\"  ‚úì Non-audio positions preserve original labels\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def test_audio_at_beginning(self):\n",
    "        \"\"\"Test with audio placeholder at the start of sequence.\"\"\"\n",
    "        print(\"\\n‚úì Test 2: Audio placeholder at sequence beginning\")\n",
    "        \n",
    "        batch_size = 2\n",
    "        text_seq_len = 6\n",
    "        num_audio_tokens = 10\n",
    "        \n",
    "        audio_embeds = torch.randn(batch_size, num_audio_tokens, self.llm_dim)\n",
    "        text_embeds = torch.randn(batch_size, text_seq_len, self.llm_dim)\n",
    "        \n",
    "        # Audio at position 0\n",
    "        input_ids = torch.tensor([\n",
    "            [self.audio_token_id, 1, 2, 3, 4, 5],\n",
    "            [self.audio_token_id, 6, 7, 8, 9, 10]\n",
    "        ])\n",
    "        \n",
    "        labels = torch.tensor([\n",
    "            [100, 10, 20, 30, 40, 50],\n",
    "            [200, 60, 70, 80, 90, 100]\n",
    "        ])\n",
    "        \n",
    "        combined_embeds, combined_labels = self.insert_func(\n",
    "            audio_embeds, text_embeds, input_ids, labels, self.audio_token_id\n",
    "        )\n",
    "        \n",
    "        expected_len = text_seq_len - 1 + num_audio_tokens\n",
    "        assert combined_embeds.shape == (batch_size, expected_len, self.llm_dim), \\\n",
    "            f\"Expected shape {(batch_size, expected_len, self.llm_dim)}, got {combined_embeds.shape}\"\n",
    "        \n",
    "        # First 10 positions should be -100\n",
    "        for i in range(num_audio_tokens):\n",
    "            assert combined_labels[0, i] == -100, \\\n",
    "                f\"Audio position {i} at beginning should have -100\"\n",
    "        \n",
    "        # Check that text labels follow after audio\n",
    "        assert combined_labels[0, num_audio_tokens] == 10, \\\n",
    "            \"First text label should appear after audio\"\n",
    "        \n",
    "        print(f\"  ‚úì Audio at beginning handled correctly\")\n",
    "        print(f\"  ‚úì Text labels follow after audio embeddings\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def test_audio_at_end(self):\n",
    "        \"\"\"Test with audio placeholder at the end of sequence.\"\"\"\n",
    "        print(\"\\n‚úì Test 3: Audio placeholder at sequence end\")\n",
    "        \n",
    "        batch_size = 2\n",
    "        text_seq_len = 6\n",
    "        num_audio_tokens = 10\n",
    "        \n",
    "        audio_embeds = torch.randn(batch_size, num_audio_tokens, self.llm_dim)\n",
    "        text_embeds = torch.randn(batch_size, text_seq_len, self.llm_dim)\n",
    "        \n",
    "        # Audio at last position\n",
    "        input_ids = torch.tensor([\n",
    "            [1, 2, 3, 4, 5, self.audio_token_id],\n",
    "            [6, 7, 8, 9, 10, self.audio_token_id]\n",
    "        ])\n",
    "        \n",
    "        labels = torch.tensor([\n",
    "            [10, 20, 30, 40, 50, 100],\n",
    "            [60, 70, 80, 90, 100, 200]\n",
    "        ])\n",
    "        \n",
    "        combined_embeds, combined_labels = self.insert_func(\n",
    "            audio_embeds, text_embeds, input_ids, labels, self.audio_token_id\n",
    "        )\n",
    "        \n",
    "        expected_len = text_seq_len - 1 + num_audio_tokens\n",
    "        assert combined_embeds.shape == (batch_size, expected_len, self.llm_dim), \\\n",
    "            f\"Expected shape {(batch_size, expected_len, self.llm_dim)}, got {combined_embeds.shape}\"\n",
    "        \n",
    "        # Last 10 positions should be -100\n",
    "        for i in range(expected_len - num_audio_tokens, expected_len):\n",
    "            assert combined_labels[0, i] == -100, \\\n",
    "                f\"Audio position {i} at end should have -100\"\n",
    "        \n",
    "        # Check that text labels come before audio\n",
    "        assert combined_labels[0, 0] == 10, \"First text label preserved\"\n",
    "        assert combined_labels[0, 4] == 50, \"Last text label before audio preserved\"\n",
    "        \n",
    "        print(f\"  ‚úì Audio at end handled correctly\")\n",
    "        print(f\"  ‚úì Text labels preserved before audio\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def test_embedding_values_preserved(self):\n",
    "        \"\"\"Test that text and audio embeddings are correctly preserved.\"\"\"\n",
    "        print(\"\\n‚úì Test 4: Embedding values preservation\")\n",
    "        \n",
    "        batch_size = 1\n",
    "        text_seq_len = 4\n",
    "        num_audio_tokens = 3\n",
    "        \n",
    "        # Create embeddings with known values\n",
    "        audio_embeds = torch.ones(batch_size, num_audio_tokens, self.llm_dim) * 100\n",
    "        text_embeds = torch.ones(batch_size, text_seq_len, self.llm_dim) * 50\n",
    "        \n",
    "        # Audio at position 2\n",
    "        input_ids = torch.tensor([[1, 2, self.audio_token_id, 3]])\n",
    "        labels = torch.tensor([[10, 20, 30, 40]])\n",
    "        \n",
    "        combined_embeds, combined_labels = self.insert_func(\n",
    "            audio_embeds, text_embeds, input_ids, labels, self.audio_token_id\n",
    "        )\n",
    "        \n",
    "        # Check text embeddings before audio\n",
    "        assert torch.allclose(combined_embeds[0, 0], text_embeds[0, 0]), \\\n",
    "            \"Text embedding at position 0 should be preserved\"\n",
    "        assert torch.allclose(combined_embeds[0, 1], text_embeds[0, 1]), \\\n",
    "            \"Text embedding at position 1 should be preserved\"\n",
    "        \n",
    "        # Check audio embeddings\n",
    "        for i in range(num_audio_tokens):\n",
    "            assert torch.allclose(combined_embeds[0, 2 + i], audio_embeds[0, i]), \\\n",
    "                f\"Audio embedding {i} should be preserved at position {2 + i}\"\n",
    "        \n",
    "        # Check text embedding after audio\n",
    "        assert torch.allclose(combined_embeds[0, 5], text_embeds[0, 3]), \\\n",
    "            \"Text embedding after audio should be preserved\"\n",
    "        \n",
    "        print(f\"  ‚úì Text embeddings preserved correctly\")\n",
    "        print(f\"  ‚úì Audio embeddings inserted correctly\")\n",
    "        print(f\"  ‚úì Embedding values match expected\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def test_batch_consistency(self):\n",
    "        \"\"\"Test that batch processing is consistent.\"\"\"\n",
    "        print(\"\\n‚úì Test 5: Batch consistency\")\n",
    "        \n",
    "        batch_size = 3\n",
    "        text_seq_len = 7\n",
    "        num_audio_tokens = 15\n",
    "        \n",
    "        audio_embeds = torch.randn(batch_size, num_audio_tokens, self.llm_dim)\n",
    "        text_embeds = torch.randn(batch_size, text_seq_len, self.llm_dim)\n",
    "        \n",
    "        # Each sample has audio at position 3\n",
    "        input_ids = torch.tensor([\n",
    "            [1, 2, 3, self.audio_token_id, 4, 5, 6],\n",
    "            [7, 8, 9, self.audio_token_id, 10, 11, 12],\n",
    "            [13, 14, 15, self.audio_token_id, 16, 17, 18]\n",
    "        ])\n",
    "        \n",
    "        labels = torch.tensor([\n",
    "            [10, 20, 30, 40, 50, 60, 70],\n",
    "            [80, 90, 100, 110, 120, 130, 140],\n",
    "            [150, 160, 170, 180, 190, 200, 210]\n",
    "        ])\n",
    "        \n",
    "        combined_embeds, combined_labels = self.insert_func(\n",
    "            audio_embeds, text_embeds, input_ids, labels, self.audio_token_id\n",
    "        )\n",
    "        \n",
    "        expected_len = text_seq_len - 1 + num_audio_tokens\n",
    "        \n",
    "        # All batch items should have same length\n",
    "        assert combined_embeds.shape[0] == batch_size, \\\n",
    "            f\"Batch size should be {batch_size}\"\n",
    "        assert combined_embeds.shape[1] == expected_len, \\\n",
    "            f\"All sequences should have length {expected_len}\"\n",
    "        \n",
    "        # Check each batch item has audio masked correctly\n",
    "        for b in range(batch_size):\n",
    "            audio_start = 3\n",
    "            for i in range(audio_start, audio_start + num_audio_tokens):\n",
    "                assert combined_labels[b, i] == -100, \\\n",
    "                    f\"Batch {b}, position {i} should be -100\"\n",
    "        \n",
    "        print(f\"  ‚úì All batch items have consistent length: {expected_len}\")\n",
    "        print(f\"  ‚úì Audio masking consistent across batch\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def run_all_tests(self):\n",
    "        \"\"\"Run all tests and report results.\"\"\"\n",
    "        print(\"=\"*70)\n",
    "        print(\"Running Audio Embedding Insertion Tests\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        tests = [\n",
    "            (\"Single Audio Token Per Sample\", self.test_single_audio_token_per_sample),\n",
    "            (\"Audio at Beginning\", self.test_audio_at_beginning),\n",
    "            (\"Audio at End\", self.test_audio_at_end),\n",
    "            (\"Embedding Values Preservation\", self.test_embedding_values_preserved),\n",
    "            (\"Batch Consistency\", self.test_batch_consistency),\n",
    "        ]\n",
    "        \n",
    "        passed = 0\n",
    "        failed = 0\n",
    "        \n",
    "        for test_name, test_func in tests:\n",
    "            try:\n",
    "                result = test_func()\n",
    "                if result:\n",
    "                    passed += 1\n",
    "            except AssertionError as e:\n",
    "                print(f\"  ‚úó FAILED: {e}\")\n",
    "                failed += 1\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚úó ERROR: {e}\")\n",
    "                failed += 1\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"Test Results: {passed}/{len(tests)} passed\")\n",
    "        if failed == 0:\n",
    "            print(\"‚úÖ All tests passed! Your implementation is correct.\")\n",
    "        else:\n",
    "            print(f\"‚ùå {failed} test(s) failed. Please review your implementation.\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        return failed == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_suite = AudioEmbeddingInsertionTests(AudioLLM.insert_audio_embeds)\n",
    "test_suite.run_all_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Implementation\n",
    "* We've implemented the model class at this point, it's time to implement Audio-based QA dataset, samples for which we've generated earlier\n",
    "* Your goal, again - fill **`<TODO>`**'s\n",
    "\n",
    "##### Grading Criteria:\n",
    "* **1 point:** Completed *<TODO>* code in the `AudioInstructDataset` + demonstration of dataset samples with **correct** labels masking\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_prompt(\n",
    "        tokenizer: AutoTokenizer,\n",
    "        instruction: str,\n",
    "        response: str,\n",
    "        system_message: str = BASE_SYSTEM_MESSAGE\n",
    ") -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message}\n",
    "    ]\n",
    "    messages.append({\"role\": \"user\", \"content\": instruction})\n",
    "\n",
    "    if response is not None:\n",
    "        messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "        add_generation_prompt = False\n",
    "    else:\n",
    "        add_generation_prompt = True\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=add_generation_prompt\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "class AudioInstructDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        json_path: str,\n",
    "        tokenizer,\n",
    "        feature_extractor: WhisperFeatureExtractor,\n",
    "        max_length: int = 512,\n",
    "        is_generation_set: bool = False,\n",
    "        num_samples: int | None = None,\n",
    "    ):\n",
    "        with open(json_path, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "\n",
    "        if num_samples is not None:\n",
    "            self.data = self.data[:num_samples]\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.max_length = max_length\n",
    "        self.is_generation_set = is_generation_set\n",
    "\n",
    "        print(f\"Loaded {len(self.data)} samples from {json_path}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx) -> dict:\n",
    "        sample = self.data[idx]\n",
    "\n",
    "        # Load audio from file path and resample to 16kHz if needed\n",
    "        audio_path = sample[\"audio_path\"]\n",
    "        waveform = None #<TODO>\n",
    "\n",
    "        # Process audio with Whisper feature extractor\n",
    "        audio_array = waveform.squeeze(0).numpy()\n",
    "        audio_inputs = self.feature_extractor(\n",
    "            audio_array,\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        audio_values = audio_inputs.input_features.squeeze(0)\n",
    "\n",
    "        # Create chat-formatted prompt with instruction and response\n",
    "        instruction = f\"{AUDIO_TOKEN}Based on the given audio, answer the question: {sample['question']}\"\n",
    "        response = sample[\"answer\"]\n",
    "\n",
    "        text = create_prompt(\n",
    "            self.tokenizer,\n",
    "            instruction=instruction,\n",
    "            response=response,\n",
    "        )\n",
    "\n",
    "        # Tokenize\n",
    "        tokenized = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"longest\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = tokenized.input_ids.squeeze(0)\n",
    "        attention_mask = tokenized.attention_mask.squeeze(0)\n",
    "\n",
    "        # Mask tokens corresponding to system and user replic with -100 value\n",
    "        labels = None #<TODO>\n",
    "\n",
    "        return {\n",
    "            \"audio_values\": audio_values,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model to overfit on small dataset\n",
    "\n",
    "We've implemented model & dataset, now we need to test it:\n",
    "* Instantiate model\n",
    "* Move it to device\n",
    "* Create dataset with small number of samples (10, for example)\n",
    "* Create trainer according to seminar notebook\n",
    "* Adjust hyperparams to reach `loss < 0.2`\n",
    "\n",
    "##### Grading Criteria:\n",
    "* **+1 point:** Reaching `loss < 0.2` on your overfit experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Model instantiation\n",
    "config = ModelConfig()\n",
    "model = AudioLLM(config)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    dtype = torch.bfloat16\n",
    "\n",
    "    # Move all components to GPU\n",
    "    model.llm = model.llm.to(device=device)\n",
    "    model.whisper_encoder = model.whisper_encoder.to(device=device, dtype=dtype)\n",
    "    model.audio_adapter = model.audio_adapter.to(device=device, dtype=dtype)\n",
    "\n",
    "    print(f\"Model moved to device: {device}, dtype: {dtype}\")\n",
    "else:\n",
    "    print(\"Using CPU (no CUDA available)\")\n",
    "\n",
    "\n",
    "tokenizer = model.tokenizer\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(config.whisper_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = AudioInstructDataset(\n",
    "    json_path = None, # Path to your QA-saved Data with specified format,\n",
    "    tokenizer = tokenizer,\n",
    "    feature_extractor = feature_extractor,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## Use Trainer to overfit on small sample\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## Use Trainer to overfit on small sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## Plot training loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3. Pretrained AudioLLM Evaluation (4 points)\n",
    "\n",
    "In this section you'll need to: \n",
    "* Load pretrained checkpoint with architecture you've implemented\n",
    "\n",
    "* Write generation method for the model\n",
    "\n",
    "* Measure the quality of the model on MMLU dataset with *answer probability* - based accuracy\n",
    "\n",
    "\n",
    "#### Grading Criteria:\n",
    "* **+2 points:** Completing `generate` function and meaningfull answers on example audios\n",
    "* **+2 point:** Completed *answer probability* - based accuracy calculation on mmlu_speech subset and reaching `accuracy > 0.4`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generation function\n",
    "First, you'll need to:\n",
    "* Implement `get_audio_values` and `generate` functions\n",
    "* Pass example audios into them with or without prompts and demonstrate meaningfull answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Downloading audios we will use in examples\n",
    "\n",
    "! gdown https://drive.google.com/uc?id=18-9If-0WZH0cPZ9MpQEwCrABA3BrTKBh\n",
    "! gdown https://drive.google.com/uc?id=1Yv6B3BEMjjmkcK4ogcutyf6gR1QOl3kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.display.Audio('gagarin_eng.mp3')\n",
    "# When did Gagarin fly into space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.display.Audio('gagarin.mp3')\n",
    "# –í –∫–∞–∫–æ–º –≥–æ–¥—É –ì–∞–≥–∞—Ä–∏–Ω –ø–æ–ª–µ—Ç–µ–ª –≤ –∫–æ—Å–º–æ—Å?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Model instantiation\n",
    "config = ModelConfig(\n",
    "    whisper_model='openai/whisper-medium',\n",
    "    llm_model=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    adapter_hidden_dim=1536,\n",
    "    adapter_num_layers=2,\n",
    "    adapter_dropout=0.1,\n",
    "    subsample_factor=4,\n",
    ")\n",
    "model = AudioLLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "ckpt_path = hf_hub_download(\n",
    "    repo_id=\"malex26/dls-course-model\",\n",
    "    filename=\"model.ckpt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "model.load_state_dict(ckpt)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_audio_values(audio_path: str | Path, features_extractor: WhisperFeatureExtractor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Extracting audio_values tensor from given audio_path\n",
    "    \"\"\"\n",
    "    return None # <TODO>\n",
    "\n",
    "def generate(\n",
    "    model: AudioLLM, \n",
    "    audio_values: torch.Tensor,\n",
    "    user_prompt: str = \"\",\n",
    "    system_prompt: str = \"\",\n",
    "    max_new_tokens: int=128,\n",
    "    **generation_kwargs,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Standard generation logic, but with addition of Audio Embeddings\n",
    "    \"\"\"\n",
    "    model = model.eval()\n",
    "    \n",
    "    if not AUDIO_TOKEN in user_prompt:\n",
    "        # model was trained with AUDIO_TOKEN before user text\n",
    "        user_prompt = AUDIO_TOKEN + user_prompt\n",
    "\n",
    "    text = None # <TODO>\n",
    "    input_ids = None# <TODO>\n",
    "\n",
    "    with torch.no_grad(), torch.autocast(dtype=torch.bfloat16, device_type='cuda'):\n",
    "        # Obtain audio embeds\n",
    "        audio_embeds = None #<TODO>\n",
    "\n",
    "        # Obtain text embeds\n",
    "        text_embeds = None #<TODO>\n",
    "\n",
    "        # Combine them\n",
    "        combined_embeds, _ = None #<TODO>\n",
    "\n",
    "        # Create attention mask\n",
    "        attention_mask = torch.ones(\n",
    "            1, combined_embeds.shape[1],\n",
    "            device=audio_embeds.device,\n",
    "            dtype=torch.long\n",
    "        )\n",
    "\n",
    "        # Generate\n",
    "        outputs = None #<TODO>\n",
    "\n",
    "        # Decode tokens\n",
    "        generated_text = None #<TODO>\n",
    "\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "audio_values = get_audio_values('gagarin_eng.mp3', feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "res = generate(\n",
    "    model=model,\n",
    "    audio_values=audio_values,\n",
    "    user_prompt=f'{AUDIO_TOKEN}Answer the question on audio.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grading Criteria:\n",
    "* **2 points:** Completed `generate` & `get_audio_values` function and demonstrated meaningfull answers on example audios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Measure the quality of the model on MMLU dataset with *answer probability* - based accuracy\n",
    "In this punct you'll evaluate the pretrained AudioLLM model on MMLU-Speech using **probability-based answer extraction**, similar to the method in original [MMLU paper](https://arxiv.org/pdf/2009.03300).\n",
    "\n",
    "### Method\n",
    "\n",
    "Instead of generating and parsing text, directly extract the answer by:\n",
    "\n",
    "1. Create prompt ending with `\"Answer:\"`\n",
    "2. Get model's logits for the **next token**\n",
    "3. Extract probabilities for tokens `\" A\"`, `\" B\"`, `\" C\"`, `\" D\"` (note: with leading space)\n",
    "4. Select the option with **highest probability**\n",
    "\n",
    "**Your goal is**:\n",
    "* implement this evaluation method\n",
    "* measure quality on mmlu-speech dataset and reach `accuracy > 0.4` on the `high_school_psychology` subset\n",
    "\n",
    "#### Grading Criteria:\n",
    "* **2 points:** Creating probability-based validation function and obtained `accuracy > 0.4` on the `high_school_psychology` subset\n",
    "* **1 points:** Creating any validation function and obtained `accuracy > 0.3` on the `high_school_psychology` subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "! pip install datasets==2.21.0 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Dataset downloading, takes about 5 minutes in kaggle notebooks\n",
    "ds = load_dataset(\"mistralai/mmlu_speech\", split='test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

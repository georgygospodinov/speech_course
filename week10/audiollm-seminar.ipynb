{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is tested and supposted to be used in kaggle notebbok.\n",
    "\n",
    "https://www.kaggle.com/code/alexandrmaximenko/audiollm-seminar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T12:06:15.754804Z",
     "iopub.status.busy": "2025-11-24T12:06:15.754535Z",
     "iopub.status.idle": "2025-11-24T12:06:55.909532Z",
     "shell.execute_reply": "2025-11-24T12:06:55.908333Z",
     "shell.execute_reply.started": "2025-11-24T12:06:15.754776Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "from transformers import AutoModel, AutoProcessor\n",
    "from transformers.generation import GenerationConfig\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import IPython\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# There might me Errors like \n",
    "# \"AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\"\n",
    "# It's okay and won't effect notebook execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T12:06:55.911628Z",
     "iopub.status.busy": "2025-11-24T12:06:55.910986Z",
     "iopub.status.idle": "2025-11-24T12:07:04.794781Z",
     "shell.execute_reply": "2025-11-24T12:07:04.793367Z",
     "shell.execute_reply.started": "2025-11-24T12:06:55.911603Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## Downloading illustrations\n",
    "\n",
    "! gdown https://drive.google.com/uc?id=1pT7Fq-igxFG-8a1c3L26zSMHs2DHLM36\n",
    "! gdown https://drive.google.com/uc?id=188nBhpSwhxP99UflMK-zxo-V1yNIObR1\n",
    "! gdown https://drive.google.com/uc?id=1tU8OQ6wzuqK_Epq6VkIJ0nMISXu1szcT\n",
    "! gdown https://drive.google.com/uc?id=1pmK-nbP4qcQIAfoTZQJ2-mQgQKJKPXMj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook we will try to use UltraVox AudioLLM\n",
    "\n",
    "**UltraVox-v0.5 - family of Audio-Conditioned LLMs with different model sizes:** [huggingface_link](https://huggingface.co/fixie-ai/ultravox-v0_5-llama-3_2-1b), [ultravox team blogpost](https://www.ultravox.ai/blog/ultravox-v0-5-taking-the-lead-in-speech-understanding)\n",
    "\n",
    "**We are going to use smallest version of UltraVox-v0.5 family, which consits of:**\n",
    "* **WhisperAudioEncoder with about 600M parameters**\n",
    "* **Audio projector with about 40M parameters**\n",
    "* **LLaMA 3.2 1B decoder, with about 1.2B parameters**\n",
    "\n",
    "![](image_0.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try UltraVox with user-friendly pipe-interface\n",
    "\n",
    "----\n",
    "**For using UltraVox you have to authorize in HuggingFace and accept the usage rules.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T23:16:54.779631Z",
     "iopub.status.busy": "2025-11-23T23:16:54.779062Z",
     "iopub.status.idle": "2025-11-23T23:16:54.805019Z",
     "shell.execute_reply": "2025-11-23T23:16:54.804152Z",
     "shell.execute_reply.started": "2025-11-23T23:16:54.779597Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Authenticate with Hugging Face\n",
    "# You can get your token from https://huggingface.co/settings/tokens\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's download audios we will use to process with model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T23:17:05.958713Z",
     "iopub.status.busy": "2025-11-23T23:17:05.958163Z",
     "iopub.status.idle": "2025-11-23T23:17:10.479002Z",
     "shell.execute_reply": "2025-11-23T23:17:10.478198Z",
     "shell.execute_reply.started": "2025-11-23T23:17:05.958688Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Downloading audios we will use in examples\n",
    "\n",
    "! gdown https://drive.google.com/uc?id=18-9If-0WZH0cPZ9MpQEwCrABA3BrTKBh\n",
    "! gdown https://drive.google.com/uc?id=1Yv6B3BEMjjmkcK4ogcutyf6gR1QOl3kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T23:17:10.481088Z",
     "iopub.status.busy": "2025-11-23T23:17:10.480845Z",
     "iopub.status.idle": "2025-11-23T23:17:10.497234Z",
     "shell.execute_reply": "2025-11-23T23:17:10.496670Z",
     "shell.execute_reply.started": "2025-11-23T23:17:10.481065Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.display.Audio('gagarin_eng.mp3')\n",
    "# When did Gagarin fly into space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T23:17:10.498504Z",
     "iopub.status.busy": "2025-11-23T23:17:10.498197Z",
     "iopub.status.idle": "2025-11-23T23:17:10.505547Z",
     "shell.execute_reply": "2025-11-23T23:17:10.504862Z",
     "shell.execute_reply.started": "2025-11-23T23:17:10.498479Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.display.Audio('gagarin.mp3')\n",
    "# В каком году Гагарин полетел в космос?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T23:17:10.507357Z",
     "iopub.status.busy": "2025-11-23T23:17:10.507110Z",
     "iopub.status.idle": "2025-11-23T23:17:38.447459Z",
     "shell.execute_reply": "2025-11-23T23:17:38.446520Z",
     "shell.execute_reply.started": "2025-11-23T23:17:10.507340Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Pipeline initialization\n",
    "pipe = transformers.pipeline(\n",
    "    model='fixie-ai/ultravox-v0_5-llama-3_2-1b', \n",
    "    trust_remote_code=True, \n",
    "    device=DEVICE, \n",
    "    dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let use our pipeline for answering our spoken questions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T23:17:38.450329Z",
     "iopub.status.busy": "2025-11-23T23:17:38.450040Z",
     "iopub.status.idle": "2025-11-23T23:17:59.874629Z",
     "shell.execute_reply": "2025-11-23T23:17:59.873897Z",
     "shell.execute_reply.started": "2025-11-23T23:17:38.450288Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "path = \"gagarin_eng.mp3\"  # When did Gagarin fly into space?\n",
    "audio, sr = librosa.load(path, sr=16000)\n",
    "\n",
    "\n",
    "turns = [\n",
    "  {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a friendly and helpful character. You love to answer questions for people.\"\n",
    "  },\n",
    "]\n",
    "result_eng = pipe({'audio': audio, 'turns': turns, 'sampling_rate': sr}, max_new_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T23:17:59.876257Z",
     "iopub.status.busy": "2025-11-23T23:17:59.875391Z",
     "iopub.status.idle": "2025-11-23T23:17:59.880902Z",
     "shell.execute_reply": "2025-11-23T23:17:59.879972Z",
     "shell.execute_reply.started": "2025-11-23T23:17:59.876222Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(result_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T23:17:59.882998Z",
     "iopub.status.busy": "2025-11-23T23:17:59.882375Z",
     "iopub.status.idle": "2025-11-23T23:18:00.848851Z",
     "shell.execute_reply": "2025-11-23T23:18:00.848227Z",
     "shell.execute_reply.started": "2025-11-23T23:17:59.882969Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "path = \"gagarin.mp3\"  # В каком году Гагарин полетел в космос?\n",
    "audio, sr = librosa.load(path, sr=16000)\n",
    "\n",
    "\n",
    "result_ru = pipe({'audio': audio, 'turns': turns, 'sampling_rate': sr}, max_new_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T23:18:00.849808Z",
     "iopub.status.busy": "2025-11-23T23:18:00.849592Z",
     "iopub.status.idle": "2025-11-23T23:18:00.854005Z",
     "shell.execute_reply": "2025-11-23T23:18:00.853240Z",
     "shell.execute_reply.started": "2025-11-23T23:18:00.849792Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(result_ru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**As we can see, model works good enough with english speech.**\n",
    "\n",
    "**But we can't say the same for russian one.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T23:18:00.855050Z",
     "iopub.status.busy": "2025-11-23T23:18:00.854800Z",
     "iopub.status.idle": "2025-11-23T23:18:05.363036Z",
     "shell.execute_reply": "2025-11-23T23:18:05.362449Z",
     "shell.execute_reply.started": "2025-11-23T23:18:00.855034Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# As we won't use pipe later, delete it and call garbage collector to free memory\n",
    "\n",
    "print(\"Before pipe deletion:\")\n",
    "print(\"allocated:\", torch.cuda.memory_allocated() / 1024**2, \"MB\")\n",
    "print(\"reserved: \", torch.cuda.memory_reserved()  / 1024**2, \"MB\")\n",
    "print(\"=\"*50)\n",
    "print(\"Pipe Deletion..\")\n",
    "pipe.model.to('cpu')\n",
    "del pipe.model\n",
    "del pipe\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"After pipe deletion:\")\n",
    "print(\"allocated:\", torch.cuda.memory_allocated() / 1024**2, \"MB\")\n",
    "print(\"reserved: \", torch.cuda.memory_reserved()  / 1024**2, \"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try little more manual way to use it - by calling Processor and Model itself\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T23:18:05.364095Z",
     "iopub.status.busy": "2025-11-23T23:18:05.363791Z",
     "iopub.status.idle": "2025-11-23T23:18:16.717021Z",
     "shell.execute_reply": "2025-11-23T23:18:16.716150Z",
     "shell.execute_reply.started": "2025-11-23T23:18:05.364077Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initializing model and processor\n",
    "model = AutoModel.from_pretrained('fixie-ai/ultravox-v0_5-llama-3_2-1b', trust_remote_code=True)\n",
    "processor = AutoProcessor.from_pretrained('fixie-ai/ultravox-v0_5-llama-3_2-1b', trust_remote_code=True)\n",
    "\n",
    "# Move model to GPU if available\n",
    "model = model.to(torch.bfloat16).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's take a look to the model and it's modules.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T23:18:16.718085Z",
     "iopub.status.busy": "2025-11-23T23:18:16.717858Z",
     "iopub.status.idle": "2025-11-23T23:18:16.725681Z",
     "shell.execute_reply": "2025-11-23T23:18:16.725037Z",
     "shell.execute_reply.started": "2025-11-23T23:18:16.718060Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T23:18:16.726909Z",
     "iopub.status.busy": "2025-11-23T23:18:16.726587Z",
     "iopub.status.idle": "2025-11-23T23:18:16.745109Z",
     "shell.execute_reply": "2025-11-23T23:18:16.744328Z",
     "shell.execute_reply.started": "2025-11-23T23:18:16.726884Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print('Model parameters num: ', sum(p.numel() for p in model.parameters()))\n",
    "print('Model encoder parameters num: ', sum(p.numel() for p in model.audio_tower.parameters()))\n",
    "print('Model modality projector parameters num: ', sum(p.numel() for p in model.multi_modal_projector.parameters()))\n",
    "print('Model text decoder parameters num: ', sum(p.numel() for p in model.language_model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Model use** `<|audio|>` **token to handle audio tokens insertion into text embeddings.**\n",
    "\n",
    "**We will create dialogue template using this token and then use model's tokenizer to transform it into correct text representation.**\n",
    "\n",
    "**Important note - we** `pass add_generation_prompt=True` **argument to** `create_chat_template` **method for adding assistant replic start to our prompt**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T23:18:16.747786Z",
     "iopub.status.busy": "2025-11-23T23:18:16.747605Z",
     "iopub.status.idle": "2025-11-23T23:18:16.765679Z",
     "shell.execute_reply": "2025-11-23T23:18:16.764916Z",
     "shell.execute_reply.started": "2025-11-23T23:18:16.747773Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "path = \"gagarin_eng.mp3\"\n",
    "audio, sr = librosa.load(path, sr=16000)\n",
    "AUDIO_TOKEN = '<|audio|>'\n",
    "\n",
    "turns = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are the helpful and smart audio assistant\"\n",
    "    }, \n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Answer the question: {AUDIO_TOKEN}\"\n",
    "    }, \n",
    "]\n",
    "\n",
    "text = processor.tokenizer.apply_chat_template(\n",
    "    turns, \n",
    "    add_generation_prompt=True, \n",
    "    tokenize=False\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next, we will use model's processor to process audio and create appropriate input for the model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T23:18:16.766554Z",
     "iopub.status.busy": "2025-11-23T23:18:16.766296Z",
     "iopub.status.idle": "2025-11-23T23:18:16.774067Z",
     "shell.execute_reply": "2025-11-23T23:18:16.773440Z",
     "shell.execute_reply.started": "2025-11-23T23:18:16.766527Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "processor_out = processor(text=text, audio=audio, sampling_rate=16000)\n",
    "inputs = {k: v.to(DEVICE) if isinstance(v, torch.Tensor) else v for k, v in processor_out.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's take a look into processor output - what model will take as input.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T23:18:16.775145Z",
     "iopub.status.busy": "2025-11-23T23:18:16.774907Z",
     "iopub.status.idle": "2025-11-23T23:18:16.804996Z",
     "shell.execute_reply": "2025-11-23T23:18:16.804261Z",
     "shell.execute_reply.started": "2025-11-23T23:18:16.775121Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(processor_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T23:18:16.806049Z",
     "iopub.status.busy": "2025-11-23T23:18:16.805795Z",
     "iopub.status.idle": "2025-11-23T23:18:16.813596Z",
     "shell.execute_reply": "2025-11-23T23:18:16.812800Z",
     "shell.execute_reply.started": "2025-11-23T23:18:16.806032Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print('Audio values shape: ', processor_out['audio_values'].shape)\n",
    "print('='*100)\n",
    "print('Audio lens: ', processor_out['audio_lens'])\n",
    "print('='*100)\n",
    "print('Audio token len: ', processor_out['audio_token_len'])\n",
    "print('='*100)\n",
    "print('Audio token start idx: ', processor_out['audio_token_start_idx'])\n",
    "print('='*100)\n",
    "print('Input ids: ', processor_out['input_ids'])\n",
    "print('='*100)\n",
    "print('Decoded Input ids: ', processor.tokenizer.decode(processor_out['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Input Values Description\n",
    "\n",
    "##### 1. **Audio Values**\n",
    "- **Shape**: `torch.Size([1, 128, 233])`\n",
    "- **Description**: Processed audio features for the audio encoder (Whisper)\n",
    "  - Dimension 0: Batch size (1 audio file)\n",
    "  - Dimension 1: Feature dimension (128 mel-frequency bins)\n",
    "  - Dimension 2: Time frames (233 frames)\n",
    "\n",
    "##### 2. **Audio Lens**\n",
    "- **Value**: `tensor([233])`\n",
    "- **Description**: Actual length of the audio in frames (before any padding). Used to track the true audio length for each sample in the batch.\n",
    "\n",
    "##### 3. **Audio Token Len**\n",
    "- **Value**: `tensor([15])`\n",
    "- **Description**: Number of audio tokens that will be inserted into the language model sequence. This is calculated as `⌈audio_lens / (encoder_ds_factor × stack_factor)⌉`, where the audio features are downsampled and stacked to reduce sequence length. In our case - this factor is 4*4 = 16\n",
    "\n",
    "##### 4. **Audio Token Start Idx**\n",
    "- **Value**: `tensor([43])`\n",
    "- **Description**: The position in the input_ids sequence where the audio tokens begin. In this case, the audio tokens are inserted at index 43, replacing the `<|audio|>` placeholder with 15 consecutive audio token embeddings.\n",
    "\n",
    "##### 5. **Input IDs**\n",
    "- **Shape**: `torch.Size([1, 59])`\n",
    "- **Description**: Tokenized text sequence that includes:\n",
    "  - System prompt with conversation template (Llama 3.2 format)\n",
    "  - User instruction: \"Answer the question:\"\n",
    "  - 15 `<|eot_id|>` tokens (token ID: 128009) as placeholders for audio embeddings\n",
    "  - Assistant response header to prompt generation\n",
    "  \n",
    "**Decoded version** shows the chat template structure with special tokens for system/user/assistant roles. The repeated `<|eot_id|>` tokens will be replaced by actual audio embeddings during model forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](image_1.png)\n",
    "![](image_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T23:18:16.814822Z",
     "iopub.status.busy": "2025-11-23T23:18:16.814498Z",
     "iopub.status.idle": "2025-11-23T23:18:20.731387Z",
     "shell.execute_reply": "2025-11-23T23:18:20.730536Z",
     "shell.execute_reply.started": "2025-11-23T23:18:16.814781Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "res = model.generate(**inputs, max_new_tokens=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As model's** `generate` **method returns token input ids, we need to decode them into text.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T23:18:20.732965Z",
     "iopub.status.busy": "2025-11-23T23:18:20.732728Z",
     "iopub.status.idle": "2025-11-23T23:18:20.738168Z",
     "shell.execute_reply": "2025-11-23T23:18:20.737372Z",
     "shell.execute_reply.started": "2025-11-23T23:18:20.732948Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(processor.tokenizer.decode(res[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As we can see, output contains our input text and generated text (under the assistant-role replic)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T23:18:20.739352Z",
     "iopub.status.busy": "2025-11-23T23:18:20.739016Z",
     "iopub.status.idle": "2025-11-23T23:18:20.874772Z",
     "shell.execute_reply": "2025-11-23T23:18:20.873644Z",
     "shell.execute_reply.started": "2025-11-23T23:18:20.739325Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output = model.forward(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T23:18:20.875817Z",
     "iopub.status.busy": "2025-11-23T23:18:20.875563Z",
     "iopub.status.idle": "2025-11-23T23:18:20.880582Z",
     "shell.execute_reply": "2025-11-23T23:18:20.879956Z",
     "shell.execute_reply.started": "2025-11-23T23:18:20.875789Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T23:18:20.881505Z",
     "iopub.status.busy": "2025-11-23T23:18:20.881235Z",
     "iopub.status.idle": "2025-11-23T23:18:21.371234Z",
     "shell.execute_reply": "2025-11-23T23:18:21.370578Z",
     "shell.execute_reply.started": "2025-11-23T23:18:20.881487Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Free model gpu memory as we're going to use another LLM in the next module\n",
    "\n",
    "print(\"Before model deletion:\")\n",
    "print(\"allocated:\", torch.cuda.memory_allocated() / 1024**2, \"MB\")\n",
    "print(\"reserved: \", torch.cuda.memory_reserved()  / 1024**2, \"MB\")\n",
    "print(\"=\"*50)\n",
    "print(\"Model Deletion..\")\n",
    "# model.to('cpu')\n",
    "# del model\n",
    "del res\n",
    "del output\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"After model deletion:\")\n",
    "print(\"allocated:\", torch.cuda.memory_allocated() / 1024**2, \"MB\")\n",
    "print(\"reserved: \", torch.cuda.memory_reserved()  / 1024**2, \"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's check how we can generate training data\n",
    "\n",
    "Consider one popular approach - [AudioChatLLaMa](https://arxiv.org/pdf/2311.06753v2)-like alignment.\n",
    "\n",
    "![](image_4.png)\n",
    "\n",
    "### Data Generation Steps:\n",
    "\n",
    "**Input**: Standard ASR dataset with `(audio, transcript)` pairs\n",
    "\n",
    "**Step 1: Use Transcript to Generate LLM Response**\n",
    "\n",
    "Given an ASR pair, the transcript is used to prompt LLM to generate a response:\n",
    "```\n",
    "prompt = \"<s>[INST] <<SYS>>\\n{{system_prompt}}\\n<</SYS>>\\n\\n{{user_prompt}} [/INST]\"\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `{{system_prompt}}` = empty (not used)\n",
    "- `{{user_prompt}}` = the transcript from ASR data\n",
    "\n",
    "**Step 2: Create Audio-Response Pairs**\n",
    "\n",
    "The original `(audio, transcript)` pairs are transformed into `(audio, llm_response)` pairs:\n",
    "- **Input to model**: Audio\n",
    "- **Output from model**: LLM-generated response (not the transcript)\n",
    "\n",
    "\n",
    "### Key Insight:\n",
    "\n",
    "Instead of training the model to transcribe audio → text, it's trained to respond to audio content as if it understood the spoken input, making it behave more like a conversational assistant.\n",
    "\n",
    "---\n",
    "\n",
    "Let's implement this pipeline with an example.\n",
    "\n",
    "We will use:\n",
    "* LLaMA-3.2-1B as LLM\n",
    "* LibriSpeech dev subset as asr dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First - let's write code for parsing dev-clean part of Librispeech dataset, which is mounted as input in our Kaggle notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T23:18:28.225134Z",
     "iopub.status.busy": "2025-11-23T23:18:28.224852Z",
     "iopub.status.idle": "2025-11-23T23:18:34.067415Z",
     "shell.execute_reply": "2025-11-23T23:18:34.066616Z",
     "shell.execute_reply.started": "2025-11-23T23:18:28.225113Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Download librispeech dev-clean data\n",
    "! wget https://openslr.elda.org/resources/12/dev-clean.tar.gz \\\n",
    "  && tar -xf dev-clean.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T23:18:34.069339Z",
     "iopub.status.busy": "2025-11-23T23:18:34.069062Z",
     "iopub.status.idle": "2025-11-23T23:18:34.149525Z",
     "shell.execute_reply": "2025-11-23T23:18:34.148724Z",
     "shell.execute_reply.started": "2025-11-23T23:18:34.069290Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "DEV_CLEAN_ROOT = Path(\"/kaggle/working/LibriSpeech/dev-clean\")\n",
    "\n",
    "\n",
    "# Parse all transcript files into a dict: utt_id -> transcription\n",
    "def load_transcripts(root: Path) -> dict:\n",
    "    transcripts = {}\n",
    "    for trans_file in root.rglob(\"*.trans.txt\"):\n",
    "        with open(trans_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if not parts:\n",
    "                    continue\n",
    "                utt_id = parts[0]              # e.g. \"84-121123-0000\"\n",
    "                text = \" \".join(parts[1:])\n",
    "                transcripts[utt_id] = text\n",
    "    return transcripts\n",
    "\n",
    "transcripts = load_transcripts(DEV_CLEAN_ROOT)\n",
    "print(\"Loaded transcripts:\", len(transcripts))\n",
    "\n",
    "# Create JSONL with one entry per audio file\n",
    "output_json = Path(\"/kaggle/working/dev-clean.json\")\n",
    "librispeech_data = []\n",
    "for flac_path in sorted(DEV_CLEAN_ROOT.rglob(\"*.flac\")):\n",
    "    utt_id = flac_path.stem  # filename without .flac\n",
    "    text = transcripts.get(utt_id)\n",
    "    record = {\n",
    "        \"audio_path\": str(flac_path),    # full path inside Kaggle FS\n",
    "        \"transcription\": text,\n",
    "    }\n",
    "    librispeech_data.append(record)\n",
    "    \n",
    "with output_json.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(librispeech_data, f)\n",
    "        \n",
    "\n",
    "print(\"Wrote JSONL to:\", output_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T23:18:34.150609Z",
     "iopub.status.busy": "2025-11-23T23:18:34.150350Z",
     "iopub.status.idle": "2025-11-23T23:18:34.162334Z",
     "shell.execute_reply": "2025-11-23T23:18:34.161658Z",
     "shell.execute_reply.started": "2025-11-23T23:18:34.150585Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Example of data sample\n",
    "index = torch.randint(len(librispeech_data), (1,))\n",
    "print('Data sample transcription:\\n', librispeech_data[index]['transcription'])\n",
    "IPython.display.Audio(librispeech_data[index]['audio_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next, we need to write code for our LLM-Response generation.** \n",
    "\n",
    "**Here i'll provide code for transformers-based generation, but**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T23:18:34.163916Z",
     "iopub.status.busy": "2025-11-23T23:18:34.163709Z",
     "iopub.status.idle": "2025-11-23T23:18:34.172341Z",
     "shell.execute_reply": "2025-11-23T23:18:34.171591Z",
     "shell.execute_reply.started": "2025-11-23T23:18:34.163901Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T23:18:34.173219Z",
     "iopub.status.busy": "2025-11-23T23:18:34.173027Z",
     "iopub.status.idle": "2025-11-23T23:18:34.184701Z",
     "shell.execute_reply": "2025-11-23T23:18:34.184105Z",
     "shell.execute_reply.started": "2025-11-23T23:18:34.173204Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# replic end token, we'll use it to find last assistant replic\n",
    "REPLIC_END_TOKEN = processor.tokenizer.encode('<|end_header_id|>', add_special_tokens=False)[0]\n",
    "\n",
    "class InstructDataGeneratorHF:\n",
    "    def __init__(self, model_path: str = \"meta-llama/Llama-3.2-1B-Instruct\"):\n",
    "        print(f\"Loading tokenizer: {model_path}\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "        print(f\"Loading transformers model: {model_path}\")\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.bfloat16 if self.device.type == \"cuda\" else torch.float32,\n",
    "            trust_remote_code=True,\n",
    "        ).to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.generation_kwargs = dict(\n",
    "            temperature=0.8,\n",
    "            top_p=0.9,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "        )\n",
    "\n",
    "        self.system_message = (\n",
    "            \"You are a helpful assistant. \"\n",
    "            \"Keep your responses concise - maximum 50 words or 2-3 sentences.\"\n",
    "        )\n",
    "\n",
    "    def create_chat_prompt(self, transcription: str) -> str:\n",
    "        \"\"\"Create chat prompt using Direct Chat template.\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_message},\n",
    "            {\"role\": \"user\", \"content\": transcription},\n",
    "        ]\n",
    "\n",
    "        prompt = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "        return prompt\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate_single(self, transcription: str) -> str:\n",
    "        \"\"\"Generate response for a single transcription.\"\"\"\n",
    "        prompt = self.create_chat_prompt(transcription)\n",
    "\n",
    "        enc = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "        )\n",
    "        input_ids = enc[\"input_ids\"].to(self.device)\n",
    "        attention_mask = enc[\"attention_mask\"].to(self.device)\n",
    "\n",
    "        generated = self.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            **self.generation_kwargs,\n",
    "        )\n",
    "\n",
    "        # Find the last assistant replic start and decode from there\n",
    "        last_assistant_replic_start = torch.where(generated[0] == REPLIC_END_TOKEN)[0][-1]\n",
    "        output_ids = generated[0][last_assistant_replic_start:]\n",
    "\n",
    "        text = self.tokenizer.decode(\n",
    "            output_ids,\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True,\n",
    "        ).strip()\n",
    "\n",
    "        return text\n",
    "\n",
    "    def generate_batch(self, transcriptions: list[str]) -> list[str]:\n",
    "        \"\"\"Generate responses for a list of transcriptions with progress bar.\"\"\"\n",
    "        responses = []\n",
    "        for transcription in tqdm(transcriptions, desc=\"Generating responses\"):\n",
    "            response = self.generate_single(transcription)\n",
    "            responses.append(response)\n",
    "        \n",
    "        return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T23:18:35.900703Z",
     "iopub.status.busy": "2025-11-23T23:18:35.900035Z",
     "iopub.status.idle": "2025-11-23T23:18:39.325033Z",
     "shell.execute_reply": "2025-11-23T23:18:39.324089Z",
     "shell.execute_reply.started": "2025-11-23T23:18:35.900680Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "generator = InstructDataGeneratorHF(\"meta-llama/Llama-3.2-1B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T23:18:39.326769Z",
     "iopub.status.busy": "2025-11-23T23:18:39.326375Z",
     "iopub.status.idle": "2025-11-23T23:20:34.898644Z",
     "shell.execute_reply": "2025-11-23T23:20:34.897926Z",
     "shell.execute_reply.started": "2025-11-23T23:18:39.326739Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 100\n",
    "\n",
    "librispeech_data_sample = librispeech_data[:NUM_SAMPLES]\n",
    "\n",
    "transcriptions = [sample['transcription'] for sample in librispeech_data_sample]\n",
    "\n",
    "generated_responses = generator.generate_batch(transcriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T23:20:34.899667Z",
     "iopub.status.busy": "2025-11-23T23:20:34.899414Z",
     "iopub.status.idle": "2025-11-23T23:20:34.904924Z",
     "shell.execute_reply": "2025-11-23T23:20:34.904161Z",
     "shell.execute_reply.started": "2025-11-23T23:20:34.899641Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Check our instruct data samples\n",
    "index = torch.randint(NUM_SAMPLES, (1,))\n",
    "print('Transcription example: ', librispeech_data_sample[index]['transcription'])\n",
    "print('Response example: ', generated_responses[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T23:20:34.907006Z",
     "iopub.status.busy": "2025-11-23T23:20:34.906811Z",
     "iopub.status.idle": "2025-11-23T23:20:34.924848Z",
     "shell.execute_reply": "2025-11-23T23:20:34.923986Z",
     "shell.execute_reply.started": "2025-11-23T23:20:34.906990Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## Add simple filtering\n",
    "generated_samples = []\n",
    "rejected_samples = []\n",
    "rejecting_strings = [\n",
    "    \"I can't create content\",\n",
    "    \"It seems like you\",\n",
    "    \"I can't help\",\n",
    "    \"I can't provide\",\n",
    "    \"I can’t support\",\n",
    "]\n",
    "for data_sample, response in zip(librispeech_data_sample[:NUM_SAMPLES], generated_responses):\n",
    "    if not any(sub in response for sub in rejecting_strings):\n",
    "        data_sample.update({'response': response})\n",
    "        generated_samples.append(data_sample)\n",
    "    else:\n",
    "        data_sample.update({'response': response})\n",
    "        rejected_samples.append(data_sample)\n",
    "\n",
    "print('Number of samples before filtering: ', NUM_SAMPLES)\n",
    "print('Number of samples after filtering: ', len(generated_samples))\n",
    "with open('instruct_data.json', 'w') as file:\n",
    "    json.dump(generated_samples, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T23:20:34.926094Z",
     "iopub.status.busy": "2025-11-23T23:20:34.925786Z",
     "iopub.status.idle": "2025-11-23T23:20:34.944507Z",
     "shell.execute_reply": "2025-11-23T23:20:34.943719Z",
     "shell.execute_reply.started": "2025-11-23T23:20:34.926069Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Check rejected samples\n",
    "index = torch.randint(len(rejected_samples), (1,))\n",
    "print('Transcription example: ', rejected_samples[index]['transcription'])\n",
    "print('Response example: ', rejected_samples[index]['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T23:20:34.945541Z",
     "iopub.status.busy": "2025-11-23T23:20:34.945255Z",
     "iopub.status.idle": "2025-11-23T23:20:37.846736Z",
     "shell.execute_reply": "2025-11-23T23:20:37.845942Z",
     "shell.execute_reply.started": "2025-11-23T23:20:34.945516Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"allocated:\", torch.cuda.memory_allocated() / 1024**2, \"MB\")\n",
    "print(\"reserved: \", torch.cuda.memory_reserved()  / 1024**2, \"MB\")\n",
    "\n",
    "print('Removing generator...')\n",
    "print('='*100)\n",
    "generator.model.to('cpu')\n",
    "del generator.model\n",
    "del generator\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"allocated:\", torch.cuda.memory_allocated() / 1024**2, \"MB\")\n",
    "print(\"reserved: \", torch.cuda.memory_reserved()  / 1024**2, \"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training (overfitting) on Instruct data\n",
    "\n",
    "**Now we want to write simple example of AudioLLM training.**\n",
    "\n",
    "**But we don't have enough resources, so we'll write simple training pipeline and make our model overfit on small batch.**\n",
    "\n",
    "**Overfitting on small batch - usefull practice for check the correctness of your training pipeline and it's commonly used in practice.**\n",
    "\n",
    "**So, we're going to implement dataset and make out UltraVox model overfit on it using transformers `trainer` class**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AudioLLM dataset && utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T23:20:37.848185Z",
     "iopub.status.busy": "2025-11-23T23:20:37.847620Z",
     "iopub.status.idle": "2025-11-23T23:20:37.859129Z",
     "shell.execute_reply": "2025-11-23T23:20:37.858275Z",
     "shell.execute_reply.started": "2025-11-23T23:20:37.848155Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BASE_SYSTEM_MESSAGE = (\n",
    "    \"You are a helpful assistant. \"\n",
    "    \"Keep your responses concise — maximum 50 words or 2–3 sentences.\"\n",
    ")\n",
    "\n",
    "# replic end token\n",
    "REPLIC_END_TOKEN = processor.tokenizer.encode('<|end_header_id|>', add_special_tokens=False)[0]\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "def create_labels(\n",
    "    tokens: torch.Tensor,\n",
    "    special_token: int = REPLIC_END_TOKEN,\n",
    "    ignore_index: int = -100\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    tokens: LongTensor of shape [batch_size, T]\n",
    "    Returns a clone where, for each row, all positions BEFORE the last\n",
    "    occurrence of special_token  are set to `ignore_index`.\n",
    "    \n",
    "    If a row does not contain `special_id`, that row is left unchanged.\n",
    "    \"\"\"\n",
    "    assert tokens.dim() == 2, \"tokens must be 2D [batch, T]\"\n",
    "    B, T = tokens.shape\n",
    "    device = tokens.device\n",
    "\n",
    "    is_special = tokens.eq(special_token)\n",
    "\n",
    "    # Indices along sequence dimension\n",
    "    idx = torch.arange(T, device=device).unsqueeze(0)\n",
    "\n",
    "    # For non-special positions put -1, for special positions put their index\n",
    "    masked_idx = torch.where(is_special, idx, torch.full_like(idx, -1))\n",
    "    \n",
    "    last_pos = masked_idx.max(dim=1).values                  # [B]\n",
    "\n",
    "    pos = idx\n",
    "    mask = pos < last_pos.unsqueeze(1)\n",
    "\n",
    "    out = tokens.clone()\n",
    "    out[mask] = ignore_index\n",
    "    return out\n",
    "    \n",
    "\n",
    "class UltraVoxAudioInstructDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        json_path: str,\n",
    "        processor: AutoProcessor,\n",
    "        max_length: int = 256,\n",
    "        num_samples: int | None = None\n",
    "    ):\n",
    "        with open(json_path, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "\n",
    "        if num_samples is not None:\n",
    "            self.data = self.data[:num_samples]\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "\n",
    "        print(f\"Loaded {len(self.data)} samples from {json_path}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx) -> dict:\n",
    "        sample = self.data[idx]\n",
    "\n",
    "        # Load audio from file path\n",
    "        audio_path = sample[\"audio_path\"]\n",
    "        audio, sample_rate = librosa.load(audio_path)\n",
    "        if sample_rate != 16000:\n",
    "            audio = librosa.resample(audio, orig_sr=sample_rate, target_sr=16000)\n",
    "            sample_rate = 16000\n",
    "                                    \n",
    "        turns = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": BASE_SYSTEM_MESSAGE\n",
    "            }, \n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"{AUDIO_TOKEN}\"\n",
    "            }, \n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": sample[\"response\"]\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        text = processor.tokenizer.apply_chat_template(\n",
    "            turns,\n",
    "            add_generation_prompt=False,\n",
    "            tokenize=False\n",
    "        )\n",
    "\n",
    "        processor_out = processor(text=text, audio=audio, sampling_rate=16000)\n",
    "        # processor_out['audio_values'] = processor_out['audio_values'].squeeze(0)\n",
    "\n",
    "        labels = create_labels(processor_out['input_ids'])\n",
    "        processor_out.update({'labels': labels})\n",
    "        inputs = {k: v[0] for k, v in processor_out.items()}\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T23:20:37.860564Z",
     "iopub.status.busy": "2025-11-23T23:20:37.859944Z",
     "iopub.status.idle": "2025-11-23T23:20:37.876624Z",
     "shell.execute_reply": "2025-11-23T23:20:37.875881Z",
     "shell.execute_reply.started": "2025-11-23T23:20:37.860544Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize our dataset with only 10 samples\n",
    "dataset = UltraVoxAudioInstructDataset('instruct_data.json', processor, num_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T23:20:37.877479Z",
     "iopub.status.busy": "2025-11-23T23:20:37.877236Z",
     "iopub.status.idle": "2025-11-23T23:20:37.949971Z",
     "shell.execute_reply": "2025-11-23T23:20:37.949144Z",
     "shell.execute_reply.started": "2025-11-23T23:20:37.877463Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Let's look at input ids and labels in our dataset sample\n",
    "# In labels there must be only assistant replic, other tokens must be masked with -100\n",
    "idx = torch.randint(len(dataset), (1,))\n",
    "print('INPUT_IDS Decoded:')\n",
    "print(processor.tokenizer.decode(dataset[idx]['input_ids'], skip_special_tokens=False))\n",
    "print('='*150)\n",
    "print('LABELS Decoded:')\n",
    "print(processor.tokenizer.decode(torch.where(dataset[idx]['labels'] != -100, dataset[idx]['labels'], processor.tokenizer.pad_token_id), skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training cycle\n",
    "\n",
    "For implementing model training cycle we will use `trainer` from transformers.\n",
    "\n",
    "To avoid CUDA OOM we will freeze almost all parameters and train only modality adapter.\n",
    "\n",
    "Let's remind, that our goal here - check the loss to slow down, as we don't have enough resources and time to make good finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T23:20:37.952229Z",
     "iopub.status.busy": "2025-11-23T23:20:37.951969Z",
     "iopub.status.idle": "2025-11-23T23:20:39.903793Z",
     "shell.execute_reply": "2025-11-23T23:20:39.903135Z",
     "shell.execute_reply.started": "2025-11-23T23:20:37.952212Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T23:20:39.905596Z",
     "iopub.status.busy": "2025-11-23T23:20:39.904640Z",
     "iopub.status.idle": "2025-11-23T23:20:51.006316Z",
     "shell.execute_reply": "2025-11-23T23:20:51.005684Z",
     "shell.execute_reply.started": "2025-11-23T23:20:39.905576Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initializing model and processor\n",
    "model = AutoModel.from_pretrained('fixie-ai/ultravox-v0_5-llama-3_2-1b', trust_remote_code=True)\n",
    "processor = AutoProcessor.from_pretrained('fixie-ai/ultravox-v0_5-llama-3_2-1b', trust_remote_code=True)\n",
    "\n",
    "# Move model to GPU if available\n",
    "model = model.to(torch.bfloat16).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T23:20:51.007291Z",
     "iopub.status.busy": "2025-11-23T23:20:51.007083Z",
     "iopub.status.idle": "2025-11-23T23:20:51.018666Z",
     "shell.execute_reply": "2025-11-23T23:20:51.017951Z",
     "shell.execute_reply.started": "2025-11-23T23:20:51.007275Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# freezing model params except modality adapter\n",
    "def freeze_model_except_projector(model):\n",
    "    \"\"\"\n",
    "    Freeze all parameters in the Ultravox model except the multimodal projector.\n",
    "    \n",
    "    Args:\n",
    "        model: Ultravox model instance\n",
    "    \"\"\"\n",
    "    # First, freeze everything\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Then, unfreeze only the projector\n",
    "    if hasattr(model, 'multi_modal_projector'):\n",
    "        for param in model.multi_modal_projector.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(\"✓ Unfroze multi_modal_projector parameters\")\n",
    "    else:\n",
    "        print(\"⚠ Warning: 'multi_modal_projector' not found in model\")\n",
    "    \n",
    "    # Print summary of trainable parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"\\nParameter Summary:\")\n",
    "    print(f\"  Total parameters: {total_params:,}\")\n",
    "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"  Frozen parameters: {total_params - trainable_params:,}\")\n",
    "    print(f\"  Trainable percentage: {100 * trainable_params / total_params:.2f}%\")\n",
    "\n",
    "# Usage:\n",
    "freeze_model_except_projector(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T23:20:51.019586Z",
     "iopub.status.busy": "2025-11-23T23:20:51.019354Z",
     "iopub.status.idle": "2025-11-23T23:20:51.060027Z",
     "shell.execute_reply": "2025-11-23T23:20:51.059181Z",
     "shell.execute_reply.started": "2025-11-23T23:20:51.019562Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "experiment_name = 'overfit_exp'\n",
    "num_steps = 200\n",
    "lr = 1e-3\n",
    "warmup_steps = 50\n",
    "output_dir = f'experiments/{experiment_name}'\n",
    "logging_steps = 10\n",
    "save_steps = 100000\n",
    "eval_steps = 100000\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    max_steps=num_steps,\n",
    "    per_device_train_batch_size=1,\n",
    "    learning_rate=lr,\n",
    "    warmup_steps=20,\n",
    "    logging_dir=os.path.join(output_dir, \"tensorboard_logs\"),\n",
    "    logging_steps=logging_steps,\n",
    "    save_steps=save_steps,\n",
    "    eval_steps=eval_steps,\n",
    "    eval_strategy=\"no\",\n",
    "    bf16=True,\n",
    "    dataloader_num_workers=1,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    save_safetensors=False,\n",
    "    gradient_checkpointing=False,\n",
    "    ddp_find_unused_parameters=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T23:20:51.061129Z",
     "iopub.status.busy": "2025-11-23T23:20:51.060881Z",
     "iopub.status.idle": "2025-11-23T23:20:51.277259Z",
     "shell.execute_reply": "2025-11-23T23:20:51.276440Z",
     "shell.execute_reply.started": "2025-11-23T23:20:51.061103Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AudioLLMTrainer(Trainer):\n",
    "    \"\"\"Custom Trainer for AudioLLM that handles device placement correctly.\"\"\"\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        \"\"\"Override to handle audio inputs properly.\"\"\"\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "trainer = AudioLLMTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=dataset,\n",
    "    # data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T23:20:51.278520Z",
     "iopub.status.busy": "2025-11-23T23:20:51.278137Z",
     "iopub.status.idle": "2025-11-23T23:22:22.394672Z",
     "shell.execute_reply": "2025-11-23T23:22:22.393990Z",
     "shell.execute_reply.started": "2025-11-23T23:20:51.278496Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer = AudioLLMTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=dataset,\n",
    "    # data_collator=data_collator,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T23:22:22.395949Z",
     "iopub.status.busy": "2025-11-23T23:22:22.395676Z",
     "iopub.status.idle": "2025-11-23T23:22:22.664362Z",
     "shell.execute_reply": "2025-11-23T23:22:22.663621Z",
     "shell.execute_reply.started": "2025-11-23T23:22:22.395927Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logdir = \"experiments/overfit_exp/tensorboard_logs\"  # directory with event files\n",
    "ea = EventAccumulator(logdir)\n",
    "ea.Reload()\n",
    "\n",
    "# list available tags\n",
    "print(ea.Tags()[\"scalars\"])\n",
    "\n",
    "# pick one scalar\n",
    "tag = \"train/loss\"  # for example\n",
    "events = ea.Scalars(tag)\n",
    "\n",
    "steps = [e.step for e in events]\n",
    "values = [e.value for e in events]\n",
    "\n",
    "plt.plot(steps, values)\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(tag)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As we can see - training happened well and loss droped almost to 0**"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

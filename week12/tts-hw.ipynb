{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63f0a772",
   "metadata": {},
   "source": [
    "### Inference of Speech-Language Model\n",
    "\n",
    "В этой домашней работе предлагается ознакомиться с тем, как работает инференс языковой модели с обученным аудио выходом. В примере используется модель Mini-Omni, для большего понимания принципа работы модели вы можете изучить оригинальную статью: [arxiv](https://arxiv.org/abs/2408.16725). Так же можете обратиться к оригинальному коду: [github](https://github.com/gpt-omni/mini-omni). \n",
    "\n",
    "В ноутбуке [tts-hw.ipynb](tts-hw.ipynb) требуется реализовать несколько полезных фичей для поддержки батчевого инференса и использования conversational модели в режиме TTS (с форсированием текстового выхода). Не меняйте код в используемых модулях, постарайтесь не менять основной код в ноутбуке вне блоков `YOUR CODE HERE`.\n",
    "\n",
    "Задание:\n",
    "1. **3 балла**: Реализовать kv-кеширование. Возможно, вы захотите заранее предусмотреть, чтобы ваша реализация работала и для пункта 2.\n",
    "2. **3 балла**: Реализовать батчевый инференс. Продумайте, как лучше добавлять паддинги при составлении входного батча.\n",
    "3. **4 балла**: Реализовать инференс с форсированием текстового выхода, чтобы можно было использовать модель как модель TTS. Вы можете добавить свои примеры текстов для озвучки. Такой режим так же должен работать в батчевом инференсе, продумайте, как правильно дополнить создание батча в этом случае. \n",
    "\n",
    "Это задание вы можете выполнять где вам удобно - локально, с использованием google-colab (для этого в ноутбуке можно склонировать этот репозиторий и добавить в `sys.path` пути до используемых модулей) или kaggle-notebooks. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c10063",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install snac soundfile omegaconf tokenizers gdown ipython;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0c7747",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Optional, Any\n",
    "from pathlib import Path\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn import functional as F\n",
    "from omegaconf import OmegaConf as om\n",
    "from tqdm import tqdm\n",
    "from snac import SNAC\n",
    "\n",
    "from tokenizer import Tokenizer\n",
    "from snac_utils import reconscruct_snac, reconstruct_tensors\n",
    "from model import apply_rope, build_rope_cache, RMSNorm, LLaMAMLP\n",
    "from download_model import download_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b3dcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "out_dir = Path(f\"outputs\")\n",
    "if not out_dir.exists():\n",
    "    out_dir.mkdir(exist_ok=True)\n",
    "ckpt_dir = Path(f\"checkpoint\")\n",
    "\n",
    "download_model(ckpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006ca284",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KVCache(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        k_shape: Tuple[int, int, int, int],\n",
    "        v_shape: Tuple[int, int, int, int],\n",
    "        device: Optional[torch.device] = None,\n",
    "        dtype: Optional[torch.dtype] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.register_buffer(\n",
    "            \"k\", torch.zeros(k_shape, device=device, dtype=dtype), persistent=False\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"v\", torch.zeros(v_shape, device=device, dtype=dtype), persistent=False\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, input_pos: torch.Tensor, k: torch.Tensor, v: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # move the buffer to the activation dtype for when AMP is used\n",
    "        self.k = self.k.to(k.dtype)\n",
    "        self.v = self.v.to(v.dtype)\n",
    "        # update the cache\n",
    "        \n",
    "        #####\n",
    "        # YOUR CODE HERE\n",
    "        #####\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        torch.nn.init.zeros_(self.k)\n",
    "        torch.nn.init.zeros_(self.v)\n",
    "        \n",
    "\n",
    "def build_mask_cache(\n",
    "    max_seq_length: int, device: Optional[torch.device] = None\n",
    ") -> torch.Tensor:\n",
    "    ones = torch.ones((max_seq_length, max_seq_length), device=device, dtype=torch.bool)\n",
    "    return torch.tril(ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad33cd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, n_embd: int, n_head: int, n_query_groups: int, head_size: int, add_qkv_bias: bool, rope_n_elem: int, bias: bool=False) -> None:\n",
    "        super().__init__()\n",
    "        shape = (n_head + 2 * n_query_groups) * head_size\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.attn = nn.Linear(n_embd, shape, bias=add_qkv_bias)\n",
    "        # output projection\n",
    "        # if `head_size` is explicitly specified in the config, `n_emd` might not be equal to `head_size * n_head`\n",
    "        self.proj = nn.Linear(\n",
    "            head_size * n_head, n_embd, bias=bias\n",
    "        )\n",
    "        # disabled by default\n",
    "        self.kv_cache: Optional[KVCache] = None\n",
    "        self.n_head = n_head\n",
    "        self.n_query_groups = n_query_groups\n",
    "        self.head_size = head_size\n",
    "        self.rope_n_elem = rope_n_elem\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        cos: torch.Tensor,\n",
    "        sin: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None,\n",
    "        input_pos: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        B, T, C = (\n",
    "            x.size()\n",
    "        )  # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        qkv = self.attn(x)\n",
    "\n",
    "        # assemble into a number of query groups to support MHA, MQA and GQA together (see `n_query_groups`)\n",
    "        q_per_kv = self.n_head // self.n_query_groups\n",
    "        total_qkv = q_per_kv + 2  # each group has 1+ queries, 1 key, and 1 value\n",
    "        qkv = qkv.view(\n",
    "            B, T, self.n_query_groups, total_qkv, self.head_size\n",
    "        )\n",
    "        qkv = qkv.permute(0, 2, 3, 1, 4)  # (B, n_query_groups, total_qkv, T, hs)\n",
    "\n",
    "        # split batched computation into three\n",
    "        q, k, v = qkv.split((q_per_kv, 1, 1), dim=2)\n",
    "\n",
    "        # maybe repeat k and v if for the non multi-head attention cases\n",
    "        # training: flash attention requires it\n",
    "        # inference: multi-query would require a full kv cache so avoid it to limit its memory usage\n",
    "        if self.n_query_groups != self.n_head and (\n",
    "            input_pos is None or self.n_query_groups != 1\n",
    "        ):\n",
    "            k = k.expand(\n",
    "                B, self.n_query_groups, q_per_kv, T, self.head_size\n",
    "            )\n",
    "            v = v.expand(\n",
    "                B, self.n_query_groups, q_per_kv, T, self.head_size\n",
    "            )\n",
    "\n",
    "        q = q.reshape(B, -1, T, self.head_size)  # (B, nh_q, T, hs)\n",
    "        k = k.reshape(B, -1, T, self.head_size)  # (B, nh_k, T, hs)\n",
    "        v = v.reshape(B, -1, T, self.head_size)  # (B, nh_v, T, hs)\n",
    "\n",
    "        q_roped = apply_rope(q[..., : self.rope_n_elem], cos, sin)\n",
    "        k_roped = apply_rope(k[..., : self.rope_n_elem], cos, sin)\n",
    "        q = torch.cat((q_roped, q[..., self.rope_n_elem :]), dim=-1)\n",
    "        k = torch.cat((k_roped, k[..., self.rope_n_elem :]), dim=-1)\n",
    "\n",
    "        if input_pos is not None:\n",
    "            if not isinstance(self.kv_cache, KVCache):\n",
    "                raise TypeError(\"You need to call `gpt.set_kv_cache()`\")\n",
    "            k, v = self.kv_cache(input_pos, k, v)\n",
    "\n",
    "        y = self.scaled_dot_product_attention(q, k, v, mask)\n",
    "\n",
    "        y = y.reshape(\n",
    "            B, T, self.head_size * self.n_head\n",
    "        )  # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        return self.proj(y)\n",
    "\n",
    "    def scaled_dot_product_attention(\n",
    "        self,\n",
    "        q: torch.Tensor,\n",
    "        k: torch.Tensor,\n",
    "        v: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        scale = 1.0 / math.sqrt(self.head_size)\n",
    "        y = torch.nn.functional.scaled_dot_product_attention(\n",
    "            q, k, v, attn_mask=mask, dropout_p=0.0, scale=scale, is_causal=mask is None\n",
    "        )\n",
    "        return y.transpose(1, 2)\n",
    "    \n",
    "    def build_kv_cache(\n",
    "        self,\n",
    "        batch_size: int,\n",
    "        max_seq_length: int,\n",
    "        rope_cache_length: Optional[int] = None,\n",
    "        device: Optional[torch.device] = None,\n",
    "        dtype: Optional[torch.dtype] = None,\n",
    "    ) -> KVCache:\n",
    "        heads = 1 if self.n_query_groups == 1 else self.n_head\n",
    "        v_shape = (batch_size, heads, max_seq_length, self.head_size)\n",
    "        if rope_cache_length is None:\n",
    "            if self.rotary_percentage != 1.0:\n",
    "                raise TypeError(\n",
    "                    \"Please pass the `rope_cache_length=gpt.cos.size(-1)` value\"\n",
    "                )\n",
    "            k_shape = v_shape\n",
    "        else:\n",
    "            k_shape = (\n",
    "                batch_size,\n",
    "                heads,\n",
    "                max_seq_length,\n",
    "                rope_cache_length + self.head_size - self.rope_n_elem,\n",
    "            )\n",
    "        return KVCache(k_shape, v_shape, device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246ffa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd: int, norm_eps: float, n_head: int, n_query_groups: int, head_size: int, intermediate_size: int, add_qkv_bias: bool, rope_n_elem: int, bias: bool) -> None:\n",
    "        super().__init__()\n",
    "        self.norm_1 = RMSNorm(n_embd, eps=norm_eps)\n",
    "        self.attn = CausalSelfAttention(n_embd, n_head, n_query_groups, head_size, add_qkv_bias, rope_n_elem, bias)\n",
    "        self.norm_2 = RMSNorm(n_embd, eps=norm_eps)\n",
    "        self.mlp = LLaMAMLP(n_embd, intermediate_size, bias)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        cos: torch.Tensor,\n",
    "        sin: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None,\n",
    "        input_pos: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        x_normed = self.norm_1(x)\n",
    "        attention_output = self.attn(x_normed, cos, sin, mask, input_pos)\n",
    "\n",
    "        x = attention_output + x\n",
    "        x = self.mlp(self.norm_2(x)) + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bc544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_q: int,\n",
    "            n_embd: int,\n",
    "            n_layer: int,\n",
    "            n_head: int,\n",
    "            n_query_groups: int,\n",
    "            head_size: int,\n",
    "            intermediate_size: int,\n",
    "            add_qkv_bias: int,\n",
    "            bias: bool,\n",
    "            block_size: int,\n",
    "            norm_eps: float,\n",
    "            tie_word_embeddings: bool,\n",
    "            scale_embeddings: bool,\n",
    "            text_vocab_size: int,\n",
    "            audio_vocab_size: int,\n",
    "            lm_head_bias: bool,\n",
    "            rotary_percentage: float,\n",
    "            rope_condense_ratio: int,\n",
    "            rope_base: int,\n",
    "            **kwargs\n",
    "            ) -> None:\n",
    "        super().__init__()\n",
    "        self.lm_heads = nn.ModuleList([nn.Linear(n_embd, text_vocab_size, bias=lm_head_bias)] + [nn.Linear(n_embd, audio_vocab_size, bias=lm_head_bias) for _ in range(n_q)])\n",
    "        self.embeds = nn.ModuleList([nn.Embedding(text_vocab_size, n_embd)] + [nn.Embedding(audio_vocab_size, n_embd) for _ in range(n_q)])\n",
    "        self.ln = RMSNorm(n_embd, eps=norm_eps)\n",
    "        self.rope_n_elem = int(rotary_percentage * head_size)\n",
    "        self.transformer = nn.ModuleList(Block(\n",
    "            n_embd, \n",
    "            norm_eps, \n",
    "            n_head, \n",
    "            n_query_groups, \n",
    "            head_size, \n",
    "            intermediate_size, \n",
    "            add_qkv_bias, \n",
    "            self.rope_n_elem, \n",
    "            bias\n",
    "            ) for _ in range(n_layer))\n",
    "        \n",
    "        self.block_size = block_size\n",
    "        self.scale_embeddings = scale_embeddings\n",
    "        self.audio_vocab_size = audio_vocab_size\n",
    "        self.text_vocab_size = text_vocab_size\n",
    "        self.rope_condense_ratio = rope_condense_ratio\n",
    "        self.rope_base = rope_base\n",
    "        self.n_embd = n_embd\n",
    "        self.n_q = n_q\n",
    "        self.max_seq_length = block_size\n",
    "        self.mask_cache: Optional[torch.Tensor] = None\n",
    "        if tie_word_embeddings:\n",
    "            for lm_head, emb in zip(self.lm_heads, self.embeds):\n",
    "               lm_head.weight = emb.weight\n",
    "\n",
    "    @property\n",
    "    def max_seq_length(self) -> int:\n",
    "        return self._max_seq_length\n",
    "\n",
    "    @max_seq_length.setter\n",
    "    def max_seq_length(self, value: int) -> None:\n",
    "        \"\"\"\n",
    "        When doing inference, the sequences used might be shorter than the model's context length.\n",
    "        This allows setting a smaller number to avoid allocating unused memory\n",
    "        \"\"\"\n",
    "        if value > self.block_size:\n",
    "            raise ValueError(\n",
    "                f\"Cannot attend to {value}, block size is only {self.block_size}\"\n",
    "            )\n",
    "        self._max_seq_length = value\n",
    "        if not hasattr(self, \"cos\"):\n",
    "            # first call\n",
    "            cos, sin = self.rope_cache()\n",
    "            self.register_buffer(\"cos\", cos, persistent=False)\n",
    "            self.register_buffer(\"sin\", sin, persistent=False)\n",
    "        # override\n",
    "        elif value != self.cos.size(0):\n",
    "            self.cos, self.sin = self.rope_cache(device=self.cos.device)\n",
    "        # the mask and kv cache size will get updated on `set_kv_cache`. we cannot update it here because we don't know\n",
    "        # if the kv cache is expected\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        # Trigger resetting the rope-cache\n",
    "        self.cos, self.sin = self.rope_cache(device=self.cos.device)\n",
    "\n",
    "    def _init_weights(self, module: nn.Module) -> None:\n",
    "        \"\"\"Meant to be used with `gpt.apply(gpt._init_weights)`.\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        input_pos: Optional[torch.Tensor] = None,\n",
    "    ) -> List[torch.Tensor]:\n",
    "        bs, _, T = input_ids.shape\n",
    "        if self.max_seq_length < T:\n",
    "            raise ValueError(\n",
    "                f\"Cannot forward sequence of length {T}, max seq length is only {self.max_seq_length}.\"\n",
    "            )\n",
    "            \n",
    "        if input_pos is not None:  # use the kv cache\n",
    "            if self.mask_cache is None:\n",
    "                raise TypeError(\"You need to call `gpt.set_kv_cache()`\")\n",
    "            #####\n",
    "            # YOUR CODE HERE\n",
    "            #####\n",
    "            pass\n",
    "        else:\n",
    "            cos = self.cos[None, :T, :].repeat(bs, 1, 1)\n",
    "            sin = self.sin[None, :T, :].repeat(bs, 1, 1)\n",
    "            mask = None\n",
    "\n",
    "        x = 0\n",
    "        for i, emb in enumerate(self.embeds):\n",
    "            x += emb(input_ids[:, i])\n",
    "        x = x / (self.n_q + 1)\n",
    "\n",
    "        if self.scale_embeddings:\n",
    "            x = x * (self.n_embd**0.5)\n",
    "\n",
    "        for block in self.transformer:\n",
    "            x = block(x, cos, sin, mask, input_pos)\n",
    "\n",
    "        x_ori = x\n",
    "        x_ori = self.ln(x_ori)\n",
    "        \n",
    "        logits = []\n",
    "        for lm_head in self.lm_heads:\n",
    "            logits.append(lm_head(x_ori))\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def rope_cache(\n",
    "        self, device: Optional[torch.device] = None\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return build_rope_cache(\n",
    "            seq_len=self.max_seq_length,\n",
    "            n_elem=self.rope_n_elem,\n",
    "            device=device,\n",
    "            condense_ratio=self.rope_condense_ratio,\n",
    "            base=self.rope_base,\n",
    "        )\n",
    "\n",
    "    def set_kv_cache(\n",
    "        self,\n",
    "        batch_size: int,\n",
    "        rope_cache_length: Optional[int] = None,\n",
    "        device: Optional[torch.device] = None,\n",
    "        dtype: Optional[torch.dtype] = None,\n",
    "    ) -> None:\n",
    "        if rope_cache_length is None:\n",
    "            rope_cache_length = self.cos.size(-1)\n",
    "        max_seq_length = self.max_seq_length\n",
    "\n",
    "        # initialize the kv cache for all blocks\n",
    "        for block in self.transformer:\n",
    "            block.attn.kv_cache = block.attn.build_kv_cache(\n",
    "                batch_size, max_seq_length, rope_cache_length, device, dtype\n",
    "            )\n",
    "\n",
    "        if self.mask_cache is None or self.mask_cache.size(-1) != max_seq_length:\n",
    "            # passing `attn_mask` to SDPA disables the flash implementation. since we only need the mask\n",
    "            # for the kv-cache support (only during inference), we only create it in that situation\n",
    "            self.mask_cache = build_mask_cache(max_seq_length, device)\n",
    "\n",
    "    def clear_kv_cache(self) -> None:\n",
    "        self.mask_cache = None\n",
    "        for block in self.transformer:\n",
    "            block.attn.kv_cache = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7416378",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    n_q: int = 7\n",
    "    batch_size: int = 1\n",
    "    text_vocabsize: int = 151936\n",
    "    text_specialtokens: int = 64\n",
    "    audio_vocabsize: int = 4096\n",
    "    audio_specialtokens: int = 64\n",
    "    max_seq_length: int = 1024\n",
    "    temperature: float = 0.9\n",
    "    top_k: int = 1\n",
    "    top_p: float = 1.0\n",
    "    device: str = \"cuda:0\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.text_eos = self.text_vocabsize\n",
    "        self.text_pad = self.text_vocabsize + 1\n",
    "        self.text_input_bos = self.text_vocabsize + 2\n",
    "        self.text_answer_bos = self.text_vocabsize + 3\n",
    "\n",
    "        self.audio_eos = self.audio_vocabsize\n",
    "        self.audio_pad = self.audio_vocabsize + 1\n",
    "        self.audio_input_bos = self.audio_vocabsize + 2\n",
    "        self.audio_answer_bos = self.audio_vocabsize + 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82afb688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(ckpt_dir: Path, inference_config: Config):\n",
    "    snacmodel = SNAC.from_pretrained(\"hubertsiuzdak/snac_24khz\").eval().to(inference_config.device)\n",
    "    text_tokenizer = Tokenizer(ckpt_dir)\n",
    "    model_config = om.load(ckpt_dir / \"model_config.yaml\")\n",
    "    model = GPT(n_q=inference_config.n_q, **model_config)\n",
    "    state_dict = torch.load(ckpt_dir / \"model.pth\")\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "    model.to(inference_config.device).eval()\n",
    "    return model, text_tokenizer, snacmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3d5080",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_top_p(logits: torch.Tensor, top_p: float) -> torch.Tensor:\n",
    "    # Example:\n",
    "    # sorted_probs=[0.1, 0.15, 0.2, 0.25, 0.3] -> sorted_cumprobs=[0.1, 0.25, 0.45, 0.7, 1.0]\n",
    "    # sorted_indices_to_remove = [1, 1, 0, 0, 0] if top_p=0.7\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, dim=-1, descending=False)\n",
    "    cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
    "    sorted_indices_to_remove = cumulative_probs <= (1 - top_p)\n",
    "    # Keep at least 1 token always to prevent the case where no token is selected\n",
    "    # In this case the most probable one is always kept\n",
    "    sorted_indices_to_remove[..., -1:] = 0\n",
    "    indices_to_remove = sorted_indices_to_remove.scatter(\n",
    "        -1, sorted_indices, sorted_indices_to_remove\n",
    "    )\n",
    "    logits = logits.masked_fill(indices_to_remove, float(\"-inf\"))\n",
    "    return logits\n",
    "\n",
    "\n",
    "def sample(\n",
    "    logits: torch.Tensor,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: Optional[int] = None,\n",
    "    top_p: float = 1.0,\n",
    ") -> torch.Tensor:\n",
    "    if top_p < 0.0 or top_p > 1.0:\n",
    "        raise ValueError(f\"top_p must be in [0, 1], got {top_p}\")\n",
    "    # optionally crop the logits to only the top k options\n",
    "    if top_k is not None:\n",
    "        v, i = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "        # do not use `torch.where` as in nanogpt because it will repeat top-k collisions\n",
    "        logits = torch.full_like(logits, float(\"-inf\")).scatter_(-1, i, v)\n",
    "    # optionally scale the logits and sample from a probability distribution\n",
    "    if temperature > 0.0 or top_p > 0.0:\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "        # optionally crop the logits to smallest set of logits with a cumulative probability above top_p\n",
    "        if 0.0 < top_p < 1.0:\n",
    "            logits = sample_top_p(logits, top_p)\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        return torch.multinomial(probs, num_samples=1)\n",
    "    return torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "def next_token(\n",
    "    model: GPT,\n",
    "    input_ids: torch.Tensor,\n",
    "    input_pos: torch.Tensor,\n",
    "    **kwargs: Any,\n",
    ") -> torch.Tensor:\n",
    "    all_logits = model(input_ids, input_pos=input_pos)\n",
    "    new_input_ids = []\n",
    "    for logits in all_logits:\n",
    "        new_input_ids.append(sample(logits[:, -1, :], **kwargs))\n",
    "    return torch.cat(new_input_ids, dim=1)\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate(\n",
    "    model: GPT,\n",
    "    input_ids: torch.Tensor,\n",
    "    speech_input_ids: torch.Tensor,\n",
    "    *,\n",
    "    forced_input_ids: Optional[torch.Tensor] = None,\n",
    "    max_returned_tokens: int = 2048,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: Optional[int] = None,\n",
    "    top_p: float = 1.0,\n",
    "    eos_id_a: Optional[int] = None,\n",
    "    eos_id_t: Optional[int] = None,\n",
    "    pad_id_a: Optional[int] = None,\n",
    "    pad_id_t: Optional[int] = None,\n",
    "    use_kv_cache: bool = True,\n",
    "):\n",
    "    device = input_ids.device\n",
    "    bs, T = input_ids.shape\n",
    "    \n",
    "    if speech_input_ids is None:\n",
    "        speech_input_ids = torch.full(\n",
    "            (bs, model.n_q, T), pad_id_a, dtype=torch.long, device=device\n",
    "        )\n",
    "    input_ids = torch.cat([input_ids[:, None], speech_input_ids], 1)\n",
    "    generated_input_ids = torch.zeros((bs, 1 + model.n_q, 0), dtype=torch.long, device=device)\n",
    "    \n",
    "    found_eos = torch.zeros((bs, 1 + model.n_q), device=device, dtype=torch.bool)\n",
    "    pad_token = torch.tensor(\n",
    "        [pad_id_t] + [pad_id_a] * model.n_q,\n",
    "        device=device,\n",
    "        dtype=torch.long,\n",
    "    )[None, :].repeat(bs, 1)\n",
    "    eos_token = torch.tensor(\n",
    "        [eos_id_t] + [eos_id_a] * model.n_q,\n",
    "        device=device,\n",
    "        dtype=torch.long,\n",
    "    )[None, :].repeat(bs, 1)\n",
    "    \n",
    "    \n",
    "    if use_kv_cache:\n",
    "        #####\n",
    "        # YOUR CODE HERE\n",
    "        #####\n",
    "        pass\n",
    "    else:\n",
    "        input_pos = None\n",
    "    \n",
    "    pred_ids = next_token(\n",
    "        model,\n",
    "        input_ids,\n",
    "        input_pos=input_pos,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "    for step in tqdm(range(max_returned_tokens - T)):\n",
    "        if forced_input_ids is not None and forced_input_ids.shape[-1] > 1:\n",
    "            #####\n",
    "            # YOUR CODE HERE\n",
    "            #####\n",
    "            pass\n",
    "                \n",
    "        pred_ids[found_eos] = pad_token[found_eos]\n",
    "        found_eos = torch.logical_or(found_eos, pred_ids == eos_token)\n",
    "        if found_eos.all():\n",
    "            break\n",
    "        \n",
    "        generated_input_ids = torch.cat([generated_input_ids, pred_ids[..., None]], dim=-1)\n",
    "        \n",
    "        if use_kv_cache:\n",
    "            input_pos = input_pos[:, -1:] + 1\n",
    "\n",
    "        pred_ids = next_token(\n",
    "            model,\n",
    "            pred_ids[..., None] if use_kv_cache else torch.cat([input_ids, generated_input_ids], dim=-1),\n",
    "            input_pos=input_pos,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "        )\n",
    "    return generated_input_ids[:, 0], generated_input_ids[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b327428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(input_ids: List[torch.Tensor], text_pad: int, speech_input_ids: torch.Tensor, audio_pad: int, forced_input_ids: Optional[List[torch.Tensor]] = None):\n",
    "    #####\n",
    "    # YOUR CODE HERE\n",
    "    #####\n",
    "    return input_ids, forced_input_ids, speech_input_ids\n",
    "    \n",
    "\n",
    "def get_input_ids(text_tokenizer, config: Config, text: str, text_answer: Optional[str] = None):\n",
    "    text_tokens = text_tokenizer.encode(text)\n",
    "    input_ids = torch.tensor([config.text_input_bos] + text_tokens.tolist() + [config.text_eos] + [config.text_answer_bos])\n",
    "    \n",
    "    if text_answer:\n",
    "        text_answer_tokens = text_tokenizer.encode(text_answer)\n",
    "        forced_input_ids = torch.tensor(text_answer_tokens.tolist() + [config.text_eos])\n",
    "    else:\n",
    "        forced_input_ids = None\n",
    "    \n",
    "    speech_input_ids = []\n",
    "    for _ in range(config.n_q):\n",
    "        speech_input_ids.append(torch.tensor([config.audio_pad] * (len(text_tokens) + 2) + [config.audio_answer_bos]))\n",
    "    speech_input_ids = torch.stack(speech_input_ids, 0)\n",
    "    return input_ids, forced_input_ids, speech_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90df3fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unschedule_codes(delayed_codes: torch.Tensor, shifts: List[int]) -> torch.Tensor:\n",
    "    speech_len = delayed_codes.shape[-1] - sum(shifts)\n",
    "    codes = torch.zeros_like(delayed_codes[..., :speech_len])\n",
    "\n",
    "    cum_shift = 0\n",
    "    for i, shift in enumerate([0] + shifts):\n",
    "        cum_shift += shift\n",
    "        codes[:, i] = delayed_codes[:, i, cum_shift : cum_shift + speech_len]\n",
    "        \n",
    "    return codes\n",
    "\n",
    "\n",
    "def postprocess_speech_codes(\n",
    "    codes: torch.Tensor, codebook_size: int, shifts: List[int] = None\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    codes = unschedule_codes(codes, shifts)\n",
    "\n",
    "    masks = ((0 <= codes) & (codes < codebook_size)).all(1)\n",
    "    codes_clean = [codes_it[:, mask].T for codes_it, mask in zip(codes, masks)]\n",
    "    lengths = torch.tensor(\n",
    "        [len(codes_it) for codes_it in codes_clean], dtype=torch.long, device=codes.device\n",
    "    ).clamp(min=1)\n",
    "    codes_clean = pad_sequence(codes_clean, batch_first=True).transpose(1, 2)\n",
    "    if codes_clean.shape[-1] == 0:\n",
    "        codes_clean = F.pad(codes_clean, (0, 1))\n",
    "    return codes_clean, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb7cd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    batch_size=1,\n",
    "    max_seq_length=1024,\n",
    "    temperature=0.9,\n",
    "    top_k=100,\n",
    "    top_p=1.0,\n",
    "    device=device\n",
    "    )\n",
    "model, text_tokenizer, snacmodel = load_model(ckpt_dir, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31249b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def decode_output(\n",
    "    texts_in: List[str],\n",
    "    tokens_t: torch.Tensor,\n",
    "    tokens_a: torch.Tensor,\n",
    "    lengths_a: torch.Tensor,\n",
    "    save_paths: List[Path]\n",
    "    ) -> List[np.ndarray]:\n",
    "    audios = []\n",
    "    for i in range(len(tokens_t)):\n",
    "        text_tensor = tokens_t[i]\n",
    "        if config.text_eos in text_tensor:\n",
    "            text_tensor = text_tensor[:int(torch.nonzero(text_tensor == config.text_eos)[0])]\n",
    "        text_out = text_tokenizer.decode(text_tensor).strip()\n",
    "\n",
    "        audiolist = reconscruct_snac(tokens_a[i, :, :int(lengths_a[i].item())].tolist())\n",
    "        audio = reconstruct_tensors(audiolist)\n",
    "\n",
    "        audio_hat = snacmodel.decode(audio).squeeze().cpu().numpy()\n",
    "        sf.write(\n",
    "            save_paths[i],\n",
    "            audio_hat,\n",
    "            24000,\n",
    "        )\n",
    "        print(f\"input: {texts_in[i]}\")\n",
    "        print(f\"out: {text_out}\")\n",
    "        audios.append(audio_hat)\n",
    "    return audios    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a55f51",
   "metadata": {},
   "source": [
    "## 0. plain inference (0 points)\n",
    "\n",
    "Здесь ничего делать не надо, просто запустите, убедитесь, что всё работает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90e47b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.batch_size = 1\n",
    "\n",
    "texts = [\n",
    "    \"What is your name?\",\n",
    "    \"How are you feeling today?\",\n",
    "    \"Can you describe your surroundings?\",\n",
    "    \"What did you do yesterday?\",\n",
    "    \"What is your favorite book and why?\",\n",
    "    \"How do you make a cup of tea?\",\n",
    "    \"What is the weather like today?\",\n",
    "    \"Can you explain the concept of time?\",\n",
    "    \"Can you tell me a joke?\",\n",
    "    \"Tell me the history of Civil War in the US\"\n",
    "]\n",
    "\n",
    "audios = []\n",
    "for idx in range(len(texts)):\n",
    "    input_ids, _, speech_input_ids = get_input_ids(text_tokenizer, config, texts[idx])\n",
    "    tokens_t, tokens_a = generate(\n",
    "        model,\n",
    "        input_ids.to(config.device).unsqueeze(0),\n",
    "        speech_input_ids.to(config.device).unsqueeze(0),\n",
    "        max_returned_tokens=config.max_seq_length,\n",
    "        temperature=config.temperature,\n",
    "        top_k=config.top_k,\n",
    "        top_p=config.top_p,\n",
    "        eos_id_t=config.text_eos,\n",
    "        eos_id_a=config.audio_eos,\n",
    "        pad_id_t=config.text_pad,\n",
    "        pad_id_a=config.audio_pad,\n",
    "        use_kv_cache=False,\n",
    "    )\n",
    "    tokens_a, lengths = postprocess_speech_codes(tokens_a, config.audio_vocabsize, [1] * (config.n_q - 1))\n",
    "    save_paths = [(out_dir / f\"0_{idx:02d}.wav\")]\n",
    "    audios.extend(decode_output([texts[idx]], tokens_t, tokens_a, lengths, save_paths))\n",
    "    \n",
    "#for audio in audios:\n",
    "display(Audio(audios[0], rate=24000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a072710",
   "metadata": {},
   "source": [
    "## 1. KV-cache (3 points)\n",
    "\n",
    "Здесь вам нужно реализовать механизм kv-кеширования, чтобы при каждом следующем шаге декодирования не нужно было пересчитывать kv-фичи для префикса входного тензора. \n",
    "\n",
    "Заполните пропуски в классе `KVCache`, `GPT` и первый пропуск в функции `generate`:\n",
    "```python\n",
    "if use_kv_cache:\n",
    "    #####\n",
    "    # YOUR CODE HERE\n",
    "    #####\n",
    "    pass\n",
    "else:\n",
    "    input_pos = None\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee985a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.batch_size = 1\n",
    "\n",
    "texts = [\n",
    "    \"What is your name?\",\n",
    "    \"How are you feeling today?\",\n",
    "    \"Can you describe your surroundings?\",\n",
    "    \"What did you do yesterday?\",\n",
    "    \"What is your favorite book and why?\",\n",
    "    \"How do you make a cup of tea?\",\n",
    "    \"What is the weather like today?\",\n",
    "    \"Can you explain the concept of time?\",\n",
    "    \"Can you tell me a joke?\",\n",
    "    \"Tell me the history of Civil War in the US\"\n",
    "]\n",
    "\n",
    "audios = []\n",
    "for idx in range(len(texts)):\n",
    "    input_ids, _, speech_input_ids = get_input_ids(text_tokenizer, config, texts[idx])\n",
    "    \n",
    "    model.set_kv_cache(batch_size=config.batch_size, device=config.device)\n",
    "    \n",
    "    tokens_t, tokens_a = generate(\n",
    "        model,\n",
    "        input_ids.to(config.device).unsqueeze(0),\n",
    "        speech_input_ids.to(config.device).unsqueeze(0),\n",
    "        max_returned_tokens=config.max_seq_length,\n",
    "        temperature=config.temperature,\n",
    "        top_k=config.top_k,\n",
    "        top_p=config.top_p,\n",
    "        eos_id_t=config.text_eos,\n",
    "        eos_id_a=config.audio_eos,\n",
    "        pad_id_t=config.text_pad,\n",
    "        pad_id_a=config.audio_pad,\n",
    "        use_kv_cache=True,\n",
    "    )\n",
    "    tokens_a, lengths = postprocess_speech_codes(tokens_a, config.audio_vocabsize, [1] * (config.n_q - 1))\n",
    "    save_paths = [(out_dir / f\"1_{idx:02d}.wav\")]\n",
    "    audios.extend(decode_output([texts[idx]], tokens_t, tokens_a, lengths, save_paths))\n",
    "    \n",
    "    model.clear_kv_cache()\n",
    "    \n",
    "# for audio in audios:\n",
    "display(Audio(audios[0], rate=24000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2df47b",
   "metadata": {},
   "source": [
    "## 2. batch inference (3 points)\n",
    "\n",
    "В этом пункте вам нужно реализовать батчевый инференс. Подсказка: нужно уделить особое внимание тому, как составляется батч из нескольких входных примеров, и как сделать так, чтобы паддинги не учитывались моделью при генерации.\n",
    "\n",
    "Заполните пропуски в функции `collate`, убедитесь, что ваша реализация kv-cache не сломалась, код работает корректно и результаты генерации получаются адекватные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039c81aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.batch_size = 4\n",
    "\n",
    "texts = [\n",
    "    \"What is your name?\",\n",
    "    \"How are you feeling today?\",\n",
    "    \"Can you describe your surroundings?\",\n",
    "    \"What did you do yesterday?\",\n",
    "    \"What is your favorite book and why?\",\n",
    "    \"How do you make a cup of tea?\",\n",
    "    \"What is the weather like today?\",\n",
    "    \"Can you explain the concept of time?\",\n",
    "    \"Can you tell me a joke?\",\n",
    "    \"Tell me the history of Civil War in the US\"\n",
    "]\n",
    "\n",
    "audios = []\n",
    "for batch_id in range(math.ceil(len(texts) / config.batch_size)):\n",
    "    input_ids, speech_input_ids = [], []\n",
    "    ids = range(batch_id * config.batch_size, min(len(texts), (batch_id + 1) * config.batch_size))\n",
    "    for idx in ids:\n",
    "        input_ids_item, _, speech_input_ids_item = get_input_ids(text_tokenizer, config, texts[idx])\n",
    "        input_ids.append(input_ids_item)\n",
    "        speech_input_ids.append(speech_input_ids_item)\n",
    "        \n",
    "    input_ids, _, speech_input_ids = collate(input_ids, config.text_pad, speech_input_ids, config.audio_pad)\n",
    "    \n",
    "    model.set_kv_cache(batch_size=config.batch_size, device=config.device)\n",
    "    \n",
    "    tokens_t, tokens_a = generate(\n",
    "        model,\n",
    "        input_ids.to(config.device),\n",
    "        speech_input_ids.to(config.device),\n",
    "        max_returned_tokens=config.max_seq_length,\n",
    "        temperature=config.temperature,\n",
    "        top_k=config.top_k,\n",
    "        top_p=config.top_p,\n",
    "        eos_id_t=config.text_eos,\n",
    "        eos_id_a=config.audio_eos,\n",
    "        pad_id_t=config.text_pad,\n",
    "        pad_id_a=config.audio_pad,\n",
    "        use_kv_cache=True\n",
    "    )\n",
    "    tokens_a, lengths = postprocess_speech_codes(tokens_a, config.audio_vocabsize, [1] * (config.n_q - 1))\n",
    "    save_paths = [(out_dir / f\"2_{idx:02d}.wav\") for idx in ids]\n",
    "    audios.extend(decode_output([texts[idx] for idx in ids], tokens_t, tokens_a, lengths, save_paths))\n",
    "\n",
    "    model.clear_kv_cache()\n",
    "    \n",
    "# for audio in audios:\n",
    "display(Audio(audios[0], rate=24000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f86a68",
   "metadata": {},
   "source": [
    "## 3. text answer forcing (4 points)\n",
    "\n",
    "В этом задании вам нужно добиться того, чтобы модель произносила заранее написанный текст, а не генерировала текст на ходу. То есть заставить её работать в режиме синтеза.\n",
    "\n",
    "Дополните функцию `collate`, заполните пропуск в функции `generate`, убедитесь что модель способна работать в режиме синтеза, добавьте свои примеры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18828208",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.batch_size = 4\n",
    "\n",
    "text_answers = [\"Some very beatiful text voiced by a stupid model\", \"The majestic toaster hummed a quiet symphony of forgotten melodies while floating above a sea of confused calculators.\"]\n",
    "texts = [\"Write arbitrary text.\"] * len(text_answers)\n",
    "\n",
    "audios = []\n",
    "for step in range(math.ceil(len(texts) / config.batch_size)):\n",
    "    input_ids, forced_input_ids, speech_input_ids = [], [], []\n",
    "    ids = range(step * config.batch_size, min(len(texts), (step + 1) * config.batch_size))\n",
    "    for idx in ids:\n",
    "        input_ids_item, forced_input_ids_item, speech_input_ids_item = get_input_ids(text_tokenizer, config, texts[idx], text_answers[idx])\n",
    "        input_ids.append(input_ids_item)\n",
    "        forced_input_ids.append(forced_input_ids_item)\n",
    "        speech_input_ids.append(speech_input_ids_item)\n",
    "        \n",
    "    input_ids, forced_input_ids, speech_input_ids = collate(input_ids, config.text_pad, speech_input_ids, config.audio_pad, forced_input_ids)\n",
    "    \n",
    "    model.set_kv_cache(batch_size=config.batch_size, device=config.device)\n",
    "    \n",
    "    tokens_t, tokens_a = generate(\n",
    "        model,\n",
    "        input_ids.to(config.device),\n",
    "        speech_input_ids.to(config.device),\n",
    "        forced_input_ids=forced_input_ids.to(config.device),\n",
    "        max_returned_tokens=config.max_seq_length,\n",
    "        temperature=config.temperature,\n",
    "        top_k=config.top_k,\n",
    "        top_p=config.top_p,\n",
    "        eos_id_t=config.text_eos,\n",
    "        eos_id_a=config.audio_eos,\n",
    "        pad_id_t=config.text_pad,\n",
    "        pad_id_a=config.audio_pad,\n",
    "        use_kv_cache=True,\n",
    "    )\n",
    "    tokens_a, lengths = postprocess_speech_codes(tokens_a, config.audio_vocabsize, [1] * (config.n_q - 1))\n",
    "    save_paths = [(out_dir / f\"4_{i:02d}.wav\") for i in ids]\n",
    "    audios.extend(decode_output([texts[i] for i in ids], tokens_t, tokens_a, lengths, save_paths))\n",
    "\n",
    "    model.clear_kv_cache()\n",
    "    \n",
    "for audio in audios:\n",
    "    display(Audio(audios[0], rate=24000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

model:
  init_weights: null

  encoder:
    dropout: 0.0
    feat_in: 64
    stride: 4
    d_model: 256
    n_layers: 8
    n_heads: 8
    ff_exp_factor: 2
    kernel_size: 7
    

  decoder:
    d_model: ${model.encoder.d_model}
    n_heads: 8
    embedding_dim: 256
    n_layers: 2
    dropout: 0.0
    tokenizer: 

optim:
  optimizer:
    name: 
      Adam
    params:
      lr: 1e-3
  scheduler:
    name:
      CosineAnnealing
    params:
      warmup_steps: 100
      max_steps: ${trainer.max_steps}


train_dataloader:
  num_workers: 32
  batch_size: 32
  prefetch_factor: 4
  dataset:
    manifest_name: train_opus/manifest.jsonl
    tokenizer: ${model.decoder.tokenizer}
    max_duration: 16.7
    min_duration: 0.1
    max_len: 68
    transforms:
      - name: mel_spectrogram
        params:
          sample_rate: 16000
          n_fft: 400
          win_length: 400
          hop_length: 160
          n_mels: ${model.encoder.feat_in}
      - name: log_scaler

val_dataloader:
  num_workers: ${train_dataloader.num_workers}
  batch_size: ${train_dataloader.batch_size}
  prefetch_factor: ${train_dataloader.prefetch_factor}
  dataset:
    manifest_name: test_opus/crowd/manifest.jsonl
    tokenizer: ${model.decoder.tokenizer}
    transforms:
      ${train_dataloader.dataset.transforms}

trainer:
  resume_from_checkpoint: null
  val_check_interval: 0.1
  check_val_every_n_epoch: 1
  log_every_n_steps: 5
  precision: 32
  accumulate_grad_batches: 1
  gradient_clip_val: 1.0
  accelerator: auto
  max_steps: 10000
  devices: 1
